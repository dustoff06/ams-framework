{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89df9e82-0324-4c10-8282-9f05429eb6b3",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5c986f-ab35-4b6b-b07d-3ce847249b66",
   "metadata": {},
   "source": [
    "This code block sets up the foundation for the Adaptive Polynomial Surrogate (APS) framework by importing all necessary dependencies and establishing GPU/CPU interoperability.\n",
    "\n",
    "It begins with `__future__` annotations for forward compatibility and integrates the RAPIDS Memory Manager (RMM) CuPy allocator, ensuring efficient GPU memory handling. Standard Python libraries such as **os**, **math**, **time**, **warnings**, **re**, and **dataclasses** provide core functionality, while typing hints improve code readability and maintainability.\n",
    "\n",
    "For array and dataframe processing, the block incorporates **CuPy** (a GPU-accelerated NumPy replacement) and **cuDF** (a GPU-based pandas equivalent), including its pandas-compatibility layer. Deep learning capabilities are enabled through **PyTorch**, with support for neural network modules, functional layers, and DLPack utilities that ensure interoperability between frameworks. Gradient-boosting models are brought in via **XGBoost** and **CatBoost**, while **cuML** provides polynomial feature engineering and GPU-accelerated train–test splitting.\n",
    "\n",
    "To maintain cleaner execution, Python warnings are suppressed, and a type alias `_Array` is defined to standardize array-like inputs across CuPy and PyTorch. A unified namespace `xp` is set to CuPy, and two utility functions are introduced:  \n",
    "- **asxp**, which ensures arrays are converted to the active backend (CuPy in this setup).  \n",
    "- **kfold_folds**, which generates randomized k-fold cross-validation splits using multiple fallback RNG methods for reproducibility.\n",
    "\n",
    "Finally, cuDF type-checking utilities (**is_numeric_dtype**, **is_datetime_dtype**, **is_bool_dtype**) are imported to support data validation during preprocessing.\n",
    "\n",
    "Overall, this initialization block establishes the computational environment, backend consistency, and baseline functionality required for APS to operate seamlessly across GPUs and CPUs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f9a5c6c-f900-4548-95d3-b09016d74852",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "# --- RMM init FIRST (before any GPU allocations) ---\n",
    "import rmm, cupy as cp\n",
    "from rmm.allocators.cupy import rmm_cupy_allocator\n",
    "\n",
    "# Safer pool sizing: ~80% of free VRAM at start\n",
    "free_mem, total_mem = cp.cuda.runtime.memGetInfo()\n",
    "pool_bytes = int(3072)\n",
    "rmm.reinitialize(pool_allocator=True, initial_pool_size=pool_bytes)\n",
    "\n",
    "# --- Arrays / DataFrames ---\n",
    "cp.cuda.set_allocator(rmm_cupy_allocator)\n",
    "cp.cuda.set_pinned_memory_allocator(cp.cuda.PinnedMemoryPool().malloc)\n",
    "_ = cp.zeros(1, dtype=cp.float32)  # optional warmup\n",
    "\n",
    "import cudf\n",
    "from cudf.api.types import is_numeric_dtype, is_datetime_dtype, is_bool_dtype\n",
    "\n",
    "# --- Torch ---\n",
    "import torch\n",
    "from rmm.allocators.torch import rmm_torch_allocator\n",
    "if hasattr(torch.cuda, \"memory\") and hasattr(torch.cuda.memory, \"change_current_allocator\"):\n",
    "    torch.cuda.memory.change_current_allocator(rmm_torch_allocator)\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.utils.dlpack as torch_dlpack  # single DLPack import\n",
    "\n",
    "# --- Tree baselines (GPU) ---\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostRegressor, Pool as CatPool\n",
    "\n",
    "# --- RAPIDS helpers ---\n",
    "from cuml.preprocessing import PolynomialFeatures as cuPolynomialFeatures\n",
    "from cuml.model_selection import train_test_split as cu_train_test_split\n",
    "from cuml.ensemble import RandomForestRegressor as cuRF\n",
    "\n",
    "# --- Stdlib & typing ---\n",
    "import os, math, time, warnings, re\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Dict, Any, Tuple, Iterable, List, Union\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Types / interop\n",
    "_Array = Union[cp.ndarray, torch.Tensor]\n",
    "xp = cp\n",
    "\n",
    "def asxp(a, dtype=None):\n",
    "    return a if isinstance(a, cp.ndarray) else cp.asarray(a, dtype=dtype)\n",
    "\n",
    "def kfold_folds(n: int, cv_folds: int, seed: int = 42):\n",
    "    k = int(min(cv_folds, max(2, n // 10)))\n",
    "    try:\n",
    "        idx = cp.random.default_rng(seed).permutation(n)\n",
    "    except Exception:\n",
    "        try:\n",
    "            idx = cp.random.RandomState(seed).permutation(n)\n",
    "        except Exception:\n",
    "            cp.random.seed(seed)\n",
    "            idx = cp.random.permutation(n)\n",
    "    return cp.array_split(idx, k)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ad8e75-23b2-4df5-8401-b42b217608d7",
   "metadata": {},
   "source": [
    "# Polynomial Features for Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "833b282d-3b91-4c51-afeb-975710837adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpu_poly_features(X_cp: cp.ndarray, degree: int, include_bias: bool = False):\n",
    "    \"\"\"\n",
    "    Build polynomial features up to `degree` on GPU - SHARED FUNCTION\n",
    "    \"\"\"\n",
    "    n, d = int(X_cp.shape[0]), int(X_cp.shape[1])\n",
    "    \n",
    "    # Pre-calculate total features\n",
    "    n_features = d  # degree 1\n",
    "    if degree >= 2:\n",
    "        n_features += d + (d * (d - 1)) // 2\n",
    "    if degree >= 3:\n",
    "        n_features += d + d * (d - 1) + (d * (d - 1) * (d - 2)) // 6\n",
    "    if include_bias:\n",
    "        n_features += 1\n",
    "    \n",
    "    # Pre-allocate and fill in-place (from previous optimization)\n",
    "    X_poly = cp.empty((n, n_features), dtype=X_cp.dtype)\n",
    "    names = []\n",
    "    idx = 0\n",
    "    \n",
    "    if include_bias:\n",
    "        X_poly[:, idx] = 1.0\n",
    "        names.append(\"1\")\n",
    "        idx += 1\n",
    "    \n",
    "    # degree 1\n",
    "    for j in range(d):\n",
    "        X_poly[:, idx] = X_cp[:, j]\n",
    "        names.append(f\"x{j}\")\n",
    "        idx += 1\n",
    "    \n",
    "    if degree >= 2:\n",
    "        for j in range(d):\n",
    "            X_poly[:, idx] = X_cp[:, j] ** 2\n",
    "            names.append(f\"x{j}^2\")\n",
    "            idx += 1\n",
    "        for a in range(d):\n",
    "            for b in range(a + 1, d):\n",
    "                X_poly[:, idx] = X_cp[:, a] * X_cp[:, b]\n",
    "                names.append(f\"x{a} x{b}\")\n",
    "                idx += 1\n",
    "    \n",
    "    if degree >= 3:\n",
    "        for j in range(d):\n",
    "            X_poly[:, idx] = X_cp[:, j] ** 3\n",
    "            names.append(f\"x{j}^3\")\n",
    "            idx += 1\n",
    "        for a in range(d):\n",
    "            x_a_sq = X_cp[:, a] ** 2\n",
    "            for b in range(d):\n",
    "                if b == a:\n",
    "                    continue\n",
    "                X_poly[:, idx] = x_a_sq * X_cp[:, b]\n",
    "                names.append(f\"x{a}^2 x{b}\")\n",
    "                idx += 1\n",
    "        for a in range(d):\n",
    "            for b in range(a + 1, d):\n",
    "                for c in range(b + 1, d):\n",
    "                    X_poly[:, idx] = X_cp[:, a] * X_cp[:, b] * X_cp[:, c]\n",
    "                    names.append(f\"x{a} x{b} x{c}\")\n",
    "                    idx += 1\n",
    "    \n",
    "    return X_poly, names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436ac408-ba10-4a32-9c90-05017954e697",
   "metadata": {},
   "source": [
    "# Fast CV Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a790cba-82e6-4e52-bb3e-cde9f827aa3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this ONCE after the polynomial features function (around line 150)\n",
    "def _fast_cv_score(X_cp, y_cp, cv_folds=3):\n",
    "    \"\"\"Fast CV R² using pre-allocated folds - SHARED FUNCTION\"\"\"\n",
    "    n, p = int(X_cp.shape[0]), int(X_cp.shape[1])\n",
    "    if p >= 0.8 * n or n < 15:\n",
    "        return -10.0\n",
    "\n",
    "    k = int(min(cv_folds, max(2, n // 10)))\n",
    "    \n",
    "    # Create folds ONCE and reuse indices\n",
    "    cp.random.seed(42)\n",
    "    idx = cp.arange(n)\n",
    "    cp.random.shuffle(idx)  # In-place shuffle instead of permutation\n",
    "    \n",
    "    fold_size = n // k\n",
    "    r2s = []\n",
    "    \n",
    "    for i in range(k):\n",
    "        # Use slicing instead of fancy indexing where possible\n",
    "        val_start = i * fold_size\n",
    "        val_end = (i + 1) * fold_size if i < k - 1 else n\n",
    "        \n",
    "        # Create boolean mask for faster indexing\n",
    "        val_mask = cp.zeros(n, dtype=bool)\n",
    "        val_mask[val_start:val_end] = True\n",
    "        train_mask = ~val_mask\n",
    "        \n",
    "        X_tr = X_cp[train_mask]\n",
    "        y_tr = y_cp[train_mask]\n",
    "        X_va = X_cp[val_mask]\n",
    "        y_va = y_cp[val_mask]\n",
    "        \n",
    "        # Fast linear regression\n",
    "        lr = TorchLinearRegression(fit_intercept=True, device=\"cuda\")\n",
    "        lr.fit(X_tr, y_tr)\n",
    "        y_hat = lr.predict(X_va)\n",
    "        \n",
    "        y_mean = cp.mean(y_va, axis=0, keepdims=True)\n",
    "        ss_res = cp.sum((y_va - y_hat) ** 2, axis=0)\n",
    "        ss_tot = cp.sum((y_va - y_mean) ** 2, axis=0) + 1e-12\n",
    "        r2 = 1.0 - (ss_res / ss_tot)\n",
    "        r2_scalar = cp.mean(r2)\n",
    "        \n",
    "        if bool(cp.isfinite(r2_scalar).item()):\n",
    "            r2s.append(r2_scalar)\n",
    "    \n",
    "    return float(cp.mean(cp.asarray(r2s, dtype=cp.float32)).item()) if r2s else -10.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00181ef2-8387-4a0a-8666-b9cc3ecb632d",
   "metadata": {},
   "source": [
    "# Forward Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b726fa2-9397-4892-8543-824e671a3436",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _vectorized_forward_selection(X_cand, y_cp, tolerance=1e-4, verbose=False):\n",
    "    \"\"\"Greedy forward selection using GPU-backed CV R² - SHARED FUNCTION\"\"\"\n",
    "    if X_cand.shape[1] == 0:\n",
    "        return []\n",
    "\n",
    "    n_candidates = X_cand.shape[1]\n",
    "    selected = []\n",
    "    remaining = list(range(n_candidates))\n",
    "    best_score = -cp.inf\n",
    "    no_improve = 0\n",
    "\n",
    "    # More aggressive limits (from previous optimization)\n",
    "    n = int(X_cand.shape[0])\n",
    "    if n < 15:\n",
    "        return selected\n",
    "\n",
    "    while (remaining and no_improve < 2 and len(selected) < min(15, n_candidates) \n",
    "           and len(selected) < n // 10):\n",
    "        \n",
    "        scores = _batch_evaluate_candidates(X_cand, y_cp, selected, remaining)\n",
    "        if len(scores) == 0:\n",
    "            break\n",
    "\n",
    "        best_idx = int(cp.argmax(scores))\n",
    "        best_cand = remaining[best_idx]\n",
    "        cand_score = float(scores[best_idx])\n",
    "\n",
    "        if cand_score > best_score + tolerance:\n",
    "            selected.append(best_cand)\n",
    "            remaining.pop(best_idx)\n",
    "            best_score = cand_score\n",
    "            no_improve = 0\n",
    "            if verbose:\n",
    "                print(f\"Selected feature {best_cand}, CV R²={best_score:.4f}\")\n",
    "        else:\n",
    "            no_improve += 1\n",
    "\n",
    "    return selected\n",
    "\n",
    "def _batch_evaluate_candidates(X_cand, y_cp, selected, remaining):\n",
    "    \"\"\"Evaluate batch of candidates - SHARED FUNCTION\"\"\"\n",
    "    scores = []\n",
    "    X_sel = X_cand[:, selected] if selected else None\n",
    "\n",
    "    for cand in remaining:\n",
    "        X_test = X_cand[:, [cand]] if X_sel is None else cp.concatenate([X_sel, X_cand[:, [cand]]], axis=1)\n",
    "        score = _fast_cv_score(X_test, y_cp, cv_folds=3)\n",
    "        scores.append(score)\n",
    "\n",
    "    return cp.asarray(scores, dtype=cp.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8cb903-255f-44be-97d7-033013be95df",
   "metadata": {},
   "source": [
    "# Memory Management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0e06bc-0e86-4840-aea9-170b08816e18",
   "metadata": {},
   "source": [
    "This snippet provides two helper functions for managing and monitoring GPU memory when working with CuPy.\n",
    "\n",
    "The `gpu_cleanup` function forces GPU memory cleanup by synchronizing the current CUDA stream and releasing all blocks from the CuPy default memory pool. If `verbose=True`, it will confirm whether cleanup completed successfully or print an error message if it fails.\n",
    "\n",
    "The `gpu_memory_check` function reports current GPU memory usage. It queries the active CUDA device for free and total memory, calculates used memory, and prints the status in megabytes along with an optional label. It returns the amount of free memory in MB. If the check fails, it prints an error and returns zero.\n",
    "\n",
    "A quick test at the end prints the installed CuPy version and attempts to display the GPU device’s free and total memory, allowing immediate confirmation that memory reporting works.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a795ca4b-0039-4a00-90c3-b1a6ffcd8992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CuPy version: 13.5.1\n",
      "Device memory: 16302.6MB free / 14865.0MB total\n"
     ]
    }
   ],
   "source": [
    "def gpu_cleanup(verbose=False):\n",
    "    \"\"\"Force GPU memory cleanup.\"\"\"\n",
    "    try:\n",
    "        cp.cuda.get_current_stream().synchronize()\n",
    "        cp.get_default_memory_pool().free_all_blocks()\n",
    "        if verbose:\n",
    "            print(\"GPU cleanup completed\")\n",
    "    except Exception as e:\n",
    "        if verbose:\n",
    "            print(f\"GPU cleanup failed: {e}\")\n",
    "\n",
    "def gpu_memory_check(label=\"\"):\n",
    "    \"\"\"Quick GPU memory status check using device memory info.\"\"\"\n",
    "    try:\n",
    "        device = cp.cuda.Device()\n",
    "        free_bytes, total_bytes = device.mem_info\n",
    "        used_bytes = total_bytes - free_bytes\n",
    "        \n",
    "        used_mb = used_bytes / (1024**2)\n",
    "        free_mb = free_bytes / (1024**2)\n",
    "        total_mb = total_bytes / (1024**2)\n",
    "        \n",
    "        print(f\"GPU Memory {label}: {used_mb:.1f}MB used, {free_mb:.1f}MB free ({total_mb:.1f}MB total)\")\n",
    "        return free_mb\n",
    "    except Exception as e:\n",
    "        print(f\"GPU memory check failed: {e}\")\n",
    "        return 0\n",
    "\n",
    "# Test the memory reporting\n",
    "print(f\"CuPy version: {cp.__version__}\")\n",
    "try:\n",
    "    device = cp.cuda.Device()\n",
    "    total, free = device.mem_info\n",
    "    print(f\"Device memory: {free/(1024**2):.1f}MB free / {total/(1024**2):.1f}MB total\")\n",
    "except Exception as e:\n",
    "    print(f\"Device memory check failed: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac67749f-9f11-4ae2-a4a3-85c312abe66e",
   "metadata": {},
   "source": [
    "# Generate Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85ad9ac-d449-4feb-bd82-5265d7dce2f7",
   "metadata": {},
   "source": [
    "The `VectorizedBinaryPolynomialFunction` class is the engine for GPU-accelerated synthetic data generation and coefficient recovery within APS experiments. It provides a suite of static methods that enable batch creation of experimental datasets with continuous and binary features, configurable functional forms (additive, multiplicative, or mixed), and flexible interaction structures.\n",
    "\n",
    "The `generate_batch_mixed_data` method supports multiple experiments at once by grouping similar configurations, seeding the GPU random number generator for reproducibility, and generating feature–response pairs `(X, y_true, y_noisy)` directly on the GPU. Internally, `_group_similar_configs` clusters experiments based on function type, number of features, polynomial degree, and interaction types, ensuring that generation logic can be reused efficiently. For individual datasets, `_generate_single_experiment` samples continuous and binary features on the GPU, combines them, and produces outcomes using additive, multiplicative, or hybrid response models that incorporate both main effects and specified interactions. Noise can be injected directly on the GPU, with safeguards in place to keep outputs non-negative when required.\n",
    "\n",
    "The class also defines helper routines for generating continuous and binary features, along with vectorized GPU implementations of additive, multiplicative, and mixed response functions. To support interpretability and validation, the method `get_true_marginal_coefficients_fixed` attempts to recover the underlying “true” marginal coefficients for each dataset. In additive models, this can often be derived analytically, while in multiplicative or mixed models the recovery is performed by fitting a polynomial model in GPU memory, with log-space adjustments applied when necessary.\n",
    "\n",
    "Together, these methods provide a fast, scalable, and consistent framework for producing synthetic experiments that capture complex real-world feature–outcome relationships while still remaining tractable for surrogate modeling and coefficient comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "726d11b3-a4ce-4fa2-bcf5-101763161876",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorizedBinaryPolynomialFunction:\n",
    "    \"\"\"Vectorized polynomial function generator for batch experiment processing\"\"\"\n",
    "    \n",
    "# requires: import cupy as cp\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_batch_mixed_data(experiment_configs, base_seed: int = 42):\n",
    "        # set up fast CuPy allocators\n",
    "        cp.cuda.set_allocator(cp.cuda.MemoryPool().malloc)\n",
    "        cp.cuda.set_pinned_memory_allocator(cp.cuda.PinnedMemoryPool().malloc)\n",
    "        _ = cp.random.random((1,), dtype=cp.float32)  # warm up\n",
    "    \n",
    "        print(f\"🔄 Generating data for {len(experiment_configs)} experiments on GPU (CuPy)…\")\n",
    "    \n",
    "        batch_data = []\n",
    "    \n",
    "        # Group configs (unchanged)\n",
    "        config_groups = VectorizedBinaryPolynomialFunction._group_similar_configs(experiment_configs)\n",
    "    \n",
    "        for group_idx, (_, configs) in enumerate(config_groups.items()):\n",
    "            if group_idx % 5 == 0:\n",
    "                print(f\"  Processing group {group_idx+1}/{len(config_groups)}\")\n",
    "    \n",
    "            for config in configs:\n",
    "                seed = int(base_seed + int(config[\"experiment_id\"]))\n",
    "                cp.random.seed(seed)\n",
    "    \n",
    "                try:\n",
    "                    # We convert outputs to CuPy so downstream stays on GPU.\n",
    "                    X, y_true, y_noisy, feature_info = VectorizedBinaryPolynomialFunction._generate_single_experiment(config)\n",
    "    \n",
    "                    X = cp.asarray(X, dtype=cp.float32)\n",
    "                    y_true = cp.asarray(y_true, dtype=cp.float32)\n",
    "                    y_noisy = cp.asarray(y_noisy, dtype=cp.float32)\n",
    "    \n",
    "                    batch_data.append((X, y_true, y_noisy, feature_info, config))\n",
    "    \n",
    "                except Exception as e:\n",
    "                    if group_idx % 10 == 0:\n",
    "                        print(f\"    Error in experiment {config['experiment_id']}: {str(e)[:50]}\")\n",
    "                    continue\n",
    "    \n",
    "        cp.cuda.Stream.null.synchronize()\n",
    "        print(f\"✅ Generated data for {len(batch_data)} valid experiments (GPU)\")\n",
    "        return batch_data\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def _group_similar_configs(configs):\n",
    "        \"\"\"Group configurations by similar parameters for batch processing\"\"\"\n",
    "        groups = {}\n",
    "        for config in configs:\n",
    "            # Create grouping key based on data generation parameters\n",
    "            key = (\n",
    "                config['function_type'],\n",
    "                config['n_continuous'], \n",
    "                config['n_binary'],\n",
    "                config['degree'],\n",
    "                tuple(sorted(config['interaction_types']))  # Sort for consistent grouping\n",
    "            )\n",
    "            \n",
    "            if key not in groups:\n",
    "                groups[key] = []\n",
    "            groups[key].append(config)\n",
    "        \n",
    "        return groups\n",
    "    \n",
    "# requires: import cupy as cp\n",
    "\n",
    "    @staticmethod\n",
    "    def _generate_single_experiment(config):\n",
    "        \"\"\"Generate data for a single experiment on GPU (CuPy).\"\"\"\n",
    "        n_samples        = int(config['n_samples'])\n",
    "        n_continuous     = int(config['n_continuous'])\n",
    "        n_binary         = int(config['n_binary'])\n",
    "        degree           = int(config['degree'])  # kept for parity; not directly used here\n",
    "        noise_level      = float(config['noise_level'])\n",
    "        function_type    = config['function_type']\n",
    "        binary_strength  = float(config['binary_effect_strength'])\n",
    "        interaction_types= config['interaction_types']\n",
    "    \n",
    "        dt = cp.float32\n",
    "    \n",
    "        # --- Generate features on GPU ---\n",
    "        if n_continuous > 0:\n",
    "            if function_type == 'additive':\n",
    "                X_cont = cp.random.uniform(-1.0, 1.0, size=(n_samples, n_continuous)).astype(dt)\n",
    "            else:  # multiplicative or mixed\n",
    "                X_cont = cp.random.uniform(0.1, 2.0, size=(n_samples, n_continuous)).astype(dt)\n",
    "        else:\n",
    "            X_cont = cp.empty((n_samples, 0), dtype=dt)\n",
    "    \n",
    "        if n_binary > 0:\n",
    "            # p=0.4 Bernoulli\n",
    "            X_bin = cp.random.binomial(1, 0.4, size=(n_samples, n_binary)).astype(dt)\n",
    "        else:\n",
    "            X_bin = cp.empty((n_samples, 0), dtype=dt)\n",
    "    \n",
    "        # --- Combine features ---\n",
    "        if n_continuous > 0 and n_binary > 0:\n",
    "            X = cp.concatenate([X_cont, X_bin], axis=1)\n",
    "        elif n_continuous > 0:\n",
    "            X = X_cont\n",
    "        elif n_binary > 0:\n",
    "            X = X_bin\n",
    "        else:\n",
    "            raise ValueError(\"Must have at least one feature\")\n",
    "    \n",
    "        # --- Response generation on GPU ---\n",
    "        if function_type == 'additive':\n",
    "            y = cp.zeros(n_samples, dtype=dt)\n",
    "    \n",
    "            # continuous main effects\n",
    "            if n_continuous > 0:\n",
    "                y += cp.sum(2.0 * X_cont**2 + 1.0 * X_cont, axis=1)\n",
    "    \n",
    "            # binary main effects\n",
    "            if n_binary > 0:\n",
    "                y += (binary_strength * 3.0) * cp.sum(X_bin, axis=1)\n",
    "    \n",
    "            # interactions\n",
    "            if 'continuous' in interaction_types and n_continuous > 1:\n",
    "                y += 1.5 * X_cont[:, 0] * X_cont[:, 1]\n",
    "    \n",
    "            if 'binary' in interaction_types and n_binary > 1:\n",
    "                y += (binary_strength * 2.0) * X_bin[:, 0] * X_bin[:, 1]\n",
    "    \n",
    "            if 'mixed' in interaction_types and n_continuous > 0 and n_binary > 0:\n",
    "                y += (binary_strength * 1.8) * X_cont[:, 0] * X_bin[:, 0]\n",
    "    \n",
    "            y += 5.0\n",
    "    \n",
    "        elif function_type == 'multiplicative':\n",
    "            y = cp.ones(n_samples, dtype=dt) * 2.0\n",
    "    \n",
    "            if n_continuous > 0:\n",
    "                factors = 1.0 + X_cont + 0.3 * (X_cont**2)\n",
    "                y *= cp.prod(factors, axis=1)\n",
    "    \n",
    "            if n_binary > 0:\n",
    "                bf = 1.0 + (binary_strength * 0.8) * X_bin\n",
    "                y *= cp.prod(bf, axis=1)\n",
    "    \n",
    "            if 'continuous' in interaction_types and n_continuous > 1:\n",
    "                y *= (1.0 + 0.2 * X_cont[:, 0] * X_cont[:, 1])\n",
    "    \n",
    "            if 'binary' in interaction_types and n_binary > 1:\n",
    "                y *= (1.0 + (binary_strength * 0.3) * X_bin[:, 0] * X_bin[:, 1])\n",
    "    \n",
    "            if 'mixed' in interaction_types and n_continuous > 0 and n_binary > 0:\n",
    "                y *= (1.0 + (binary_strength * 0.25) * X_cont[:, 0] * X_bin[:, 0])\n",
    "    \n",
    "            y *= 1.5\n",
    "    \n",
    "        else:  # 'mixed'\n",
    "            # additive branch\n",
    "            y_add = cp.zeros(n_samples, dtype=dt)\n",
    "            if n_continuous > 0:\n",
    "                y_add += cp.sum(2.0 * X_cont**2 + 1.0 * X_cont, axis=1)\n",
    "            if n_binary > 0:\n",
    "                y_add += (binary_strength * 3.0) * cp.sum(X_bin, axis=1)\n",
    "            if 'continuous' in interaction_types and n_continuous > 1:\n",
    "                y_add += 1.5 * X_cont[:, 0] * X_cont[:, 1]\n",
    "            if 'binary' in interaction_types and n_binary > 1:\n",
    "                y_add += (binary_strength * 2.0) * X_bin[:, 0] * X_bin[:, 1]\n",
    "            if 'mixed' in interaction_types and n_continuous > 0 and n_binary > 0:\n",
    "                y_add += (binary_strength * 1.8) * X_cont[:, 0] * X_bin[:, 0]\n",
    "            y_add += 5.0\n",
    "    \n",
    "            # multiplicative branch\n",
    "            y_mul = cp.ones(n_samples, dtype=dt) * 2.0\n",
    "            if n_continuous > 0:\n",
    "                factors = 1.0 + X_cont + 0.3 * (X_cont**2)\n",
    "                y_mul *= cp.prod(factors, axis=1)\n",
    "            if n_binary > 0:\n",
    "                bf = 1.0 + (binary_strength * 0.8) * X_bin\n",
    "                y_mul *= cp.prod(bf, axis=1)\n",
    "            if 'continuous' in interaction_types and n_continuous > 1:\n",
    "                y_mul *= (1.0 + 0.2 * X_cont[:, 0] * X_cont[:, 1])\n",
    "            if 'binary' in interaction_types and n_binary > 1:\n",
    "                y_mul *= (1.0 + (binary_strength * 0.3) * X_bin[:, 0] * X_bin[:, 1])\n",
    "            if 'mixed' in interaction_types and n_continuous > 0 and n_binary > 0:\n",
    "                y_mul *= (1.0 + (binary_strength * 0.25) * X_cont[:, 0] * X_bin[:, 0])\n",
    "            y_mul *= 1.5\n",
    "    \n",
    "            # normalize & mix\n",
    "            add_mu, add_sd = cp.mean(y_add), cp.std(y_add) + 1e-8\n",
    "            mul_mu, mul_sd = cp.mean(y_mul), cp.std(y_mul) + 1e-8\n",
    "            y_add_n = (y_add - add_mu) / add_sd\n",
    "            y_mul_n = (y_mul - mul_mu) / mul_sd\n",
    "            additive_weight = float(config.get('additive_weight', 0.5))\n",
    "            y = additive_weight * y_add_n + (1.0 - additive_weight) * y_mul_n\n",
    "            y = y * 3.0 + 10.0\n",
    "    \n",
    "        y_true = y.copy()\n",
    "    \n",
    "        # --- Noise on GPU ---\n",
    "        if noise_level > 0.0:\n",
    "            sigma = float(noise_level) * float(cp.std(y))\n",
    "            noise = cp.random.normal(0.0, sigma, size=n_samples).astype(dt)\n",
    "            y_noisy = y + noise\n",
    "        else:\n",
    "            y_noisy = y.copy()\n",
    "    \n",
    "        # --- Ensure positivity if needed ---\n",
    "        ymin = float(cp.min(y_noisy))\n",
    "        if ymin <= 0.0:\n",
    "            shift = abs(ymin) + 0.1\n",
    "            y_noisy = y_noisy + cp.asarray(shift, dtype=dt)\n",
    "            y_true  = y_true  + cp.asarray(shift, dtype=dt)\n",
    "    \n",
    "        feature_info = {\n",
    "            'n_continuous': n_continuous,\n",
    "            'n_binary': n_binary,\n",
    "            'continuous_indices': list(range(n_continuous)),\n",
    "            'binary_indices': list(range(n_continuous, n_continuous + n_binary)),\n",
    "            'interaction_types': interaction_types,\n",
    "            'binary_effect_strength': binary_strength,\n",
    "        }\n",
    "    \n",
    "        return X, y_true, y_noisy, feature_info\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def _generate_continuous_features(n_samples, n_continuous, function_type, dtype=cp.float32, rng=None):\n",
    "        \"\"\"Vectorized continuous feature generation on GPU (CuPy).\"\"\"\n",
    "        if n_continuous == 0:\n",
    "            return cp.empty((n_samples, 0), dtype=dtype)\n",
    "        rng = rng or cp.random\n",
    "        if function_type == 'additive':\n",
    "            return rng.uniform(-1.0, 1.0, size=(n_samples, n_continuous)).astype(dtype)\n",
    "        else:  # multiplicative or mixed\n",
    "            return rng.uniform(0.1, 2.0, size=(n_samples, n_continuous)).astype(dtype)\n",
    "        \n",
    "    @staticmethod\n",
    "    def _generate_binary_features(n_samples, n_binary, dtype=cp.int8, rng=None):\n",
    "        \"\"\"Vectorized binary feature generation on GPU (CuPy).\"\"\"\n",
    "        if n_binary == 0:\n",
    "            return cp.empty((n_samples, 0), dtype=dtype)\n",
    "        rng = rng or cp.random\n",
    "        return rng.binomial(1, 0.4, size=(n_samples, n_binary)).astype(dtype)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _compute_additive_response(X_cont, X_bin, binary_strength, interaction_types, dtype=cp.float32):\n",
    "        \"\"\"Vectorized additive response on GPU (CuPy).\"\"\"\n",
    "        n_samples = X_cont.shape[0] if X_cont.size else X_bin.shape[0]\n",
    "        y = cp.zeros(n_samples, dtype=dtype)\n",
    "    \n",
    "        n_cont = X_cont.shape[1]\n",
    "        n_bin  = X_bin.shape[1]\n",
    "    \n",
    "        # continuous main effects: sum(2*x^2 + 1*x)\n",
    "        if n_cont > 0:\n",
    "            y += cp.sum(2.0 * (X_cont.astype(dtype, copy=False) ** 2) +\n",
    "                        1.0 * X_cont.astype(dtype, copy=False), axis=1)\n",
    "    \n",
    "        # binary main effects: sum(binary_strength * 3 * x)\n",
    "        if n_bin > 0:\n",
    "            xb = X_bin.astype(dtype, copy=False)\n",
    "            y += (binary_strength * 3.0) * cp.sum(xb, axis=1)\n",
    "    \n",
    "        # interactions\n",
    "        if 'continuous' in interaction_types and n_cont > 1:\n",
    "            y += 1.5 * X_cont[:, 0].astype(dtype, copy=False) * X_cont[:, 1].astype(dtype, copy=False)\n",
    "    \n",
    "        if 'binary' in interaction_types and n_bin > 1:\n",
    "            xb = X_bin.astype(dtype, copy=False)\n",
    "            y += (binary_strength * 2.0) * xb[:, 0] * xb[:, 1]\n",
    "    \n",
    "        if 'mixed' in interaction_types and n_cont > 0 and n_bin > 0:\n",
    "            xb0 = X_bin[:, 0].astype(dtype, copy=False)\n",
    "            y += (binary_strength * 1.8) * X_cont[:, 0].astype(dtype, copy=False) * xb0\n",
    "    \n",
    "        # base level\n",
    "        y += 5.0\n",
    "        return y\n",
    "\n",
    "    @staticmethod\n",
    "    def _compute_multiplicative_response(X_cont, X_bin, binary_strength, interaction_types, dtype=cp.float32):\n",
    "        \"\"\"Vectorized multiplicative response on GPU (CuPy).\"\"\"\n",
    "        n_samples = X_cont.shape[0]\n",
    "        n_cont = X_cont.shape[1]\n",
    "        n_bin  = X_bin.shape[1]\n",
    "    \n",
    "        Xc = X_cont.astype(dtype, copy=False)\n",
    "        Xb = X_bin.astype(dtype, copy=False)\n",
    "    \n",
    "        y = cp.ones(n_samples, dtype=dtype) * 2.0\n",
    "    \n",
    "        # continuous multiplicative factors\n",
    "        if n_cont > 0:\n",
    "            factors = 1.0 + Xc + 0.3 * (Xc ** 2)\n",
    "            y *= cp.prod(factors, axis=1)\n",
    "    \n",
    "        # binary multiplicative factors\n",
    "        if n_bin > 0:\n",
    "            bf = 1.0 + (binary_strength * 0.8) * Xb\n",
    "            y *= cp.prod(bf, axis=1)\n",
    "    \n",
    "        # interactions\n",
    "        if 'continuous' in interaction_types and n_cont > 1:\n",
    "            y *= (1.0 + 0.2 * Xc[:, 0] * Xc[:, 1])\n",
    "    \n",
    "        if 'binary' in interaction_types and n_bin > 1:\n",
    "            y *= (1.0 + (binary_strength * 0.3) * Xb[:, 0] * Xb[:, 1])\n",
    "    \n",
    "        if 'mixed' in interaction_types and n_cont > 0 and n_bin > 0:\n",
    "            y *= (1.0 + (binary_strength * 0.25) * Xc[:, 0] * Xb[:, 0])\n",
    "    \n",
    "        y *= 1.5\n",
    "        return y\n",
    "    \n",
    "    @staticmethod\n",
    "    def _compute_mixed_response(\n",
    "        X_cont,\n",
    "        X_bin,\n",
    "        binary_strength,\n",
    "        interaction_types,\n",
    "        additive_weight: float = 0.5,\n",
    "        dtype=cp.float32,\n",
    "    ):\n",
    "        \"\"\"Vectorized mixed response on GPU (CuPy).\"\"\"\n",
    "        # compute branches on GPU\n",
    "        y_add = VectorizedBinaryPolynomialFunction._compute_additive_response(\n",
    "            X_cont, X_bin, binary_strength, interaction_types, dtype=dtype\n",
    "        ).astype(dtype, copy=False)\n",
    "    \n",
    "        y_mul = VectorizedBinaryPolynomialFunction._compute_multiplicative_response(\n",
    "            X_cont, X_bin, binary_strength, interaction_types, dtype=dtype\n",
    "        ).astype(dtype, copy=False)\n",
    "    \n",
    "        # normalize each branch\n",
    "        eps    = cp.asarray(1e-8, dtype=dtype)\n",
    "        add_mu = cp.mean(y_add)\n",
    "        add_sd = cp.std(y_add) + eps\n",
    "        mul_mu = cp.mean(y_mul)\n",
    "        mul_sd = cp.std(y_mul) + eps\n",
    "    \n",
    "        y_add_n = (y_add - add_mu) / add_sd\n",
    "        y_mul_n = (y_mul - mul_mu) / mul_sd\n",
    "    \n",
    "        # mix and shift\n",
    "        aw = float(additive_weight)\n",
    "        y_mixed = aw * y_add_n + (1.0 - aw) * y_mul_n\n",
    "        return y_mixed * 3.0 + 10.0\n",
    "\n",
    "    @staticmethod\n",
    "    def get_true_marginal_coefficients_fixed(\n",
    "        n_continuous, n_binary, binary_strength, function_type,\n",
    "        X_sample=None, y_sample=None, additive_weight=0.5, dtype=cp.float32\n",
    "    ):\n",
    "        \"\"\"\n",
    "        GPU version: estimate 'true' marginal coefficients by fitting a degree-2\n",
    "        polynomial model on GPU. For multiplicative: fit in log-space.\n",
    "        Returns a dict with intercept and main-effect (linear/quadratic) terms only.\n",
    "        \"\"\"\n",
    "        true_coeffs = {}\n",
    "        p_cont, p_bin = int(n_continuous), int(n_binary)\n",
    "        p = p_cont + p_bin\n",
    "    \n",
    "        # ----- Case 1: additive -> coefficients are known from construction -----\n",
    "        if function_type == 'additive':\n",
    "            for i in range(p_cont):\n",
    "                true_coeffs[f'x{i}_linear'] = 1.0\n",
    "                true_coeffs[f'x{i}_quadratic'] = 2.0\n",
    "            for j in range(p_bin):\n",
    "                true_coeffs[f'x{p_cont + j}_binary'] = float(binary_strength) * 3.0\n",
    "            true_coeffs['intercept'] = 5.0\n",
    "            return true_coeffs\n",
    "    \n",
    "        # If no sample provided, fall back to the original approximations\n",
    "        if X_sample is None or y_sample is None:\n",
    "            if function_type == 'multiplicative':\n",
    "                true_coeffs['intercept'] = math.log(2.0 * 1.5)\n",
    "                for i in range(p_cont):\n",
    "                    true_coeffs[f'x{i}_linear'] = 1.0\n",
    "                    true_coeffs[f'x{i}_quadratic'] = 0.3\n",
    "                for j in range(p_bin):\n",
    "                    true_coeffs[f'x{p_cont + j}_binary'] = float(binary_strength) * 0.8\n",
    "                return true_coeffs\n",
    "            else:  # mixed fallback\n",
    "                for i in range(p_cont):\n",
    "                    true_coeffs[f'x{i}_linear'] = 1.0\n",
    "                    true_coeffs[f'x{i}_quadratic'] = 0.5\n",
    "                for j in range(p_bin):\n",
    "                    true_coeffs[f'x{p_cont + j}_binary'] = float(binary_strength) * 2.0\n",
    "                true_coeffs['intercept'] = 10.0\n",
    "                return true_coeffs\n",
    "    \n",
    "        # ----- GPU path with samples -----\n",
    "        X = cp.asarray(X_sample, dtype=dtype)\n",
    "        y = cp.asarray(y_sample, dtype=dtype)\n",
    "    \n",
    "        # Build degree-2 polynomial design on GPU:\n",
    "        # Design columns = [1, x_i (all), x_i^2 (continuous only), x_i*x_j (all i<j)]\n",
    "        cols = [cp.ones((X.shape[0], 1), dtype=dtype)]  # intercept\n",
    "    \n",
    "        # linear terms (all variables)\n",
    "        cols.append(X)\n",
    "    \n",
    "        # quadratic terms for continuous only\n",
    "        if p_cont > 0:\n",
    "            cols.append(X[:, :p_cont] ** 2)\n",
    "    \n",
    "        # pairwise interactions (all i<j)\n",
    "        if p > 1:\n",
    "            inter_cols = []\n",
    "            for i in range(p):\n",
    "                Xi = X[:, i:i+1]\n",
    "                for j in range(i+1, p):\n",
    "                    inter_cols.append(Xi * X[:, j:j+1])\n",
    "            if inter_cols:\n",
    "                cols.append(cp.concatenate(inter_cols, axis=1))\n",
    "    \n",
    "        X_poly = cp.concatenate(cols, axis=1)\n",
    "    \n",
    "        # Target vector: log-space for multiplicative; original y for mixed\n",
    "        if function_type == 'multiplicative':\n",
    "            ymin = float(cp.min(y))\n",
    "            y_pos = y - ymin + 1.0 if ymin <= 0.0 else y\n",
    "            t = cp.log(y_pos)\n",
    "        else:  # 'mixed'\n",
    "            t = y\n",
    "    \n",
    "        # Solve least squares on GPU\n",
    "        # beta shape: (n_features,)\n",
    "        beta, *_ = cp.linalg.lstsq(X_poly, t, rcond=None)\n",
    "    \n",
    "        # Map back to main effects\n",
    "        # index layout:\n",
    "        #   0                      -> intercept\n",
    "        #   1 .. p                 -> linear terms x0..x{p-1}\n",
    "        #   p+1 .. p+p_cont        -> quadratic terms (continuous only): x0^2..x{p_cont-1}^2\n",
    "        #   remaining              -> interactions (ignored for marginal coeffs)\n",
    "        idx = 0\n",
    "        intercept = float(beta[idx].item()); idx += 1\n",
    "    \n",
    "        # linear\n",
    "        for i in range(p):\n",
    "            true_coeffs[f'x{i}_linear' if i < p_cont else f'x{i}_binary'] = float(beta[idx + i].item())\n",
    "        idx += p\n",
    "    \n",
    "        # quadratic (continuous only)\n",
    "        for i in range(p_cont):\n",
    "            true_coeffs[f'x{i}_quadratic'] = float(beta[idx + i].item())\n",
    "        idx += p_cont\n",
    "    \n",
    "        true_coeffs['intercept'] = intercept\n",
    "        return true_coeffs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5488e870-0d88-4452-8582-0b6ad4800df1",
   "metadata": {},
   "source": [
    "# Evaluate Marginal Coefficient Recovery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e700d2-470f-4630-974a-3d004269aff4",
   "metadata": {},
   "source": [
    "This function, **`evaluate_marginal_coefficient_recovery_fixed`**, measures how well the fitted AMS/APS model recovers the true main-effect coefficients on the GPU. It begins by checking if a model fit exists; if not, it returns default values. The training data is then moved to CuPy, the number of continuous and binary predictors is inferred, and a **main-effects-only polynomial design matrix** is constructed up to degree `D = ams_model.max_degree`. Continuous variables contribute powers \\(1..D\\) (linear, quadratic, cubic, etc.), while binary variables contribute only a single linear column (since \\(b^k = b\\), preventing collinearity). Depending on the selected method, the target vector is transformed—**MAPS** fits in log-space with a positivity shift, while **APS** uses the original space—and a least-squares solution is solved directly on the GPU. The recovered coefficients are mapped back to interpretable names (e.g., `x0_linear`, `x0_quadratic`, `x3_binary`, plus `intercept`) and compared against `true_coeffs`. Only shared, non-NaN terms are included in the comparison. The function then computes **sign accuracy**, **absolute-magnitude correlation** (with a stable fallback for single coefficients), and **RMSE** between recovered and true values. Finally, it returns these metrics along with the number of coefficients compared, providing a GPU-accelerated checkpoint for coefficient direction, magnitude, and error—fully aligned with APS/MAPS design choices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b254a13a-9ec9-469f-bb21-ba425f95648e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_marginal_coefficient_recovery_fixed(\n",
    "    ams_model,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    true_coeffs,\n",
    "    n_continuous,\n",
    "    n_binary,\n",
    "    dtype=cp.float32,\n",
    "):\n",
    "    \"\"\"\n",
    "    GPU version (CuPy):\n",
    "      - Main-effects polynomial design up to degree D = ams_model.max_degree\n",
    "      - Continuous vars: powers 1..D\n",
    "      - Binary vars: linear only (b^k == b), to avoid collinearity\n",
    "      - MAPS => fit in log-space with positive shift; APS => original space\n",
    "      - Metrics computed on GPU\n",
    "    \"\"\"\n",
    "    if getattr(ams_model, \"final_model\", None) is None:\n",
    "        return {\n",
    "            'sign_accuracy': 0.0,\n",
    "            'magnitude_correlation': 0.0,\n",
    "            'coefficient_rmse': float('inf'),\n",
    "            'n_compared': 0\n",
    "        }\n",
    "\n",
    "    # move data to GPU\n",
    "    X = cp.asarray(X_train, dtype=dtype)\n",
    "    y = cp.asarray(y_train, dtype=dtype)\n",
    "\n",
    "    p_cont = int(n_continuous)\n",
    "    p_bin  = int(n_binary)\n",
    "    p      = p_cont + p_bin\n",
    "    D      = int(getattr(ams_model, \"max_degree\", 3))\n",
    "\n",
    "    # ---- build main-effects polynomial design up to degree D ----\n",
    "    # columns: [1, (for each feature i) x_i^1, x_i^2..x_i^D (continuous only)]\n",
    "    cols = [cp.ones((X.shape[0], 1), dtype=dtype)]  # intercept\n",
    "\n",
    "    # continuous variables: powers 1..D\n",
    "    for i in range(p_cont):\n",
    "        Xi = X[:, i:i+1]\n",
    "        # stack powers (1..D)\n",
    "        powers = [Xi] + [Xi ** k for k in range(2, D + 1)]\n",
    "        cols.append(cp.concatenate(powers, axis=1))\n",
    "\n",
    "    # binary variables: linear only to avoid duplicate cols\n",
    "    for j in range(p_cont, p):\n",
    "        cols.append(X[:, j:j+1])\n",
    "\n",
    "    X_poly = cp.concatenate(cols, axis=1)\n",
    "\n",
    "    # target: MAPS => log-space with shift; APS => original\n",
    "    if getattr(ams_model, 'selected_method', 'APS') == 'MAPS':\n",
    "        ymin = float(cp.min(y))\n",
    "        y_pos = y - ymin + 1.0 if ymin <= 0.0 else y\n",
    "        t = cp.log(y_pos)\n",
    "    else:\n",
    "        t = y\n",
    "\n",
    "    # ---- solve least squares on GPU ----\n",
    "    beta, *_ = cp.linalg.lstsq(X_poly, t, rcond=None)\n",
    "\n",
    "    # ---- map coefficients back to names consistent with your dict ----\n",
    "    coeffs_rec = {}\n",
    "    idx = 0\n",
    "\n",
    "    # intercept\n",
    "    coeffs_rec['intercept'] = float(beta[idx].item()); idx += 1\n",
    "\n",
    "    # continuous vars: degrees 1..D\n",
    "    for i in range(p_cont):\n",
    "        for k in range(1, D + 1):\n",
    "            b = float(beta[idx].item()); idx += 1\n",
    "            if   k == 1: key = f'x{i}_linear'\n",
    "            elif k == 2: key = f'x{i}_quadratic'\n",
    "            elif k == 3: key = f'x{i}_cubic'\n",
    "            else:        key = f'x{i}_power{k}'\n",
    "            coeffs_rec[key] = b\n",
    "\n",
    "    # binary vars: linear only\n",
    "    for j in range(p_cont, p):\n",
    "        b = float(beta[idx].item()); idx += 1\n",
    "        coeffs_rec[f'x{j}_binary'] = b  # (linear effect for binary)\n",
    "\n",
    "    # ---- compare to provided true coefficients (GPU) ----\n",
    "    common = [k for k in coeffs_rec if (k in true_coeffs) and (true_coeffs[k] is not None)]\n",
    "    if not common:\n",
    "        return {\n",
    "            'sign_accuracy': 0.0,\n",
    "            'magnitude_correlation': 0.0,\n",
    "            'coefficient_rmse': float('inf'),\n",
    "            'n_compared': 0\n",
    "        }\n",
    "\n",
    "    tv = cp.asarray([true_coeffs[k] for k in common], dtype=cp.float64)\n",
    "    fv = cp.asarray([coeffs_rec[k]  for k in common], dtype=cp.float64)\n",
    "\n",
    "    mask = ~cp.isnan(tv)\n",
    "    if not bool(mask.any()):\n",
    "        return {\n",
    "            'sign_accuracy': 0.0,\n",
    "            'magnitude_correlation': 0.0,\n",
    "            'coefficient_rmse': float('inf'),\n",
    "            'n_compared': 0\n",
    "        }\n",
    "    tv = tv[mask]; fv = fv[mask]\n",
    "    n = int(tv.size)\n",
    "    if n == 0:\n",
    "        return {\n",
    "            'sign_accuracy': 0.0,\n",
    "            'magnitude_correlation': 0.0,\n",
    "            'coefficient_rmse': float('inf'),\n",
    "            'n_compared': 0\n",
    "        }\n",
    "\n",
    "    # sign accuracy\n",
    "    sign_acc = float(cp.mean(cp.sign(tv) == cp.sign(fv)).item())\n",
    "\n",
    "    # magnitude correlation (abs values); special-case n==1\n",
    "    if n > 1:\n",
    "        r = cp.corrcoef(cp.abs(tv), cp.abs(fv))[0, 1]\n",
    "        mag_corr = float(cp.nan_to_num(r, nan=0.0).item())\n",
    "    else:\n",
    "        rel_err = float(cp.abs(tv[0] - fv[0]) / (cp.abs(tv[0]) + 1e-10))\n",
    "        mag_corr = max(0.0, 1.0 - rel_err)\n",
    "\n",
    "    # RMSE\n",
    "    rmse = float(cp.sqrt(cp.mean((tv - fv) ** 2)).item())\n",
    "\n",
    "    return {\n",
    "        'sign_accuracy': sign_acc,\n",
    "        'magnitude_correlation': mag_corr,\n",
    "        'coefficient_rmse': rmse,\n",
    "        'n_compared': n\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2b1ee2-e878-4ecc-8c39-b43ab6a4f644",
   "metadata": {},
   "source": [
    "# Quantile Remapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285cc15e-0acf-4f88-b0ca-5bcfe029053b",
   "metadata": {},
   "source": [
    "The **`QuantileRemapper`** class provides a GPU-accelerated way to remap feature values based on their quantile distributions, improving interpretability of MAPS coefficients. It works entirely with **CuPy arrays** to leverage GPU speed. When `fit` is called, the class processes each variable, recording its original range and constructing a quantile mapping: sorted source values are paired with evenly spaced target values over the variable’s range. If a feature is constant, it simply maps to zeros. The `transform` method applies this mapping to new data by using `cp.interp`, effectively normalizing each variable into a quantile-based scale while preserving its distribution. Conversely, `inverse_transform` reverses this mapping, restoring values back to their original scale using the stored quantile functions. Internal safeguards ensure that input arrays are CuPy-based, and the class tracks whether it has been fitted before use. This remapping provides a consistent framework for comparing coefficients across heterogeneous variables, particularly important when interpreting polynomial effects in MAPS models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db3d5b0e-a622-4635-9899-b63de15523f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "\n",
    "class QuantileRemapper:\n",
    "    \"\"\"Quantile-based variable remapping for MAPS coefficient interpretability (GPU/CuPy)\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.quantile_functions = {}\n",
    "        self.original_ranges = {}\n",
    "        self.is_fitted = False\n",
    "\n",
    "    def _ensure_gpu(self, X):\n",
    "        if not isinstance(X, cp.ndarray):\n",
    "            raise TypeError(\"QuantileRemapper expects a CuPy array (cupy.ndarray).\")\n",
    "\n",
    "    def fit(self, X: cp.ndarray):\n",
    "        \"\"\"Fit quantile mapping functions for each variable on GPU\"\"\"\n",
    "        self._ensure_gpu(X)\n",
    "        self.quantile_functions = {}\n",
    "        self.original_ranges = {}\n",
    "\n",
    "        n_features = X.shape[1]\n",
    "        for i in range(n_features):\n",
    "            var_data = cp.ascontiguousarray(X[:, i])\n",
    "            vmin = var_data.min()\n",
    "            vmax = var_data.max()\n",
    "            original_range = vmax - vmin\n",
    "            self.original_ranges[i] = original_range\n",
    "\n",
    "            sorted_values = cp.sort(var_data)\n",
    "            n_unique = cp.unique(sorted_values).size\n",
    "\n",
    "            if n_unique > 1:\n",
    "                # Map quantiles to (0, original_range]\n",
    "                target_values = cp.linspace(\n",
    "                    0.01,\n",
    "                    (original_range + cp.asarray(0.01, dtype=sorted_values.dtype)),\n",
    "                    sorted_values.size,\n",
    "                    dtype=sorted_values.dtype,\n",
    "                )\n",
    "                self.quantile_functions[i] = {\n",
    "                    \"source_values\": sorted_values,\n",
    "                    \"target_values\": target_values,\n",
    "                }\n",
    "            else:\n",
    "                self.quantile_functions[i] = {\n",
    "                    \"source_values\": sorted_values,\n",
    "                    \"target_values\": cp.zeros_like(sorted_values),\n",
    "                }\n",
    "\n",
    "        self.is_fitted = True\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: cp.ndarray) -> cp.ndarray:\n",
    "        \"\"\"Apply quantile remapping to variables on GPU\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Must fit before transform\")\n",
    "        self._ensure_gpu(X)\n",
    "\n",
    "        X_remapped = cp.zeros_like(X)\n",
    "        n_features = X.shape[1]\n",
    "        for i in range(n_features):\n",
    "            var_data = cp.ascontiguousarray(X[:, i])\n",
    "            mapping = self.quantile_functions[i]\n",
    "            # cp.interp operates on 1D arrays (ascending x assumed)\n",
    "            remapped = cp.interp(\n",
    "                var_data.astype(mapping[\"source_values\"].dtype, copy=False),\n",
    "                mapping[\"source_values\"],\n",
    "                mapping[\"target_values\"],\n",
    "            )\n",
    "            X_remapped[:, i] = remapped.astype(X.dtype, copy=False)\n",
    "\n",
    "        return X_remapped\n",
    "\n",
    "    def inverse_transform(self, X_remapped: cp.ndarray) -> cp.ndarray:\n",
    "        \"\"\"Inverse quantile remapping back to original scale on GPU\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Must fit before inverse_transform\")\n",
    "        self._ensure_gpu(X_remapped)\n",
    "\n",
    "        X_original = cp.zeros_like(X_remapped)\n",
    "        n_features = X_remapped.shape[1]\n",
    "        for i in range(n_features):\n",
    "            var_data = cp.ascontiguousarray(X_remapped[:, i])\n",
    "            mapping = self.quantile_functions[i]\n",
    "            restored = cp.interp(\n",
    "                var_data.astype(mapping[\"target_values\"].dtype, copy=False),\n",
    "                mapping[\"target_values\"],\n",
    "                mapping[\"source_values\"],\n",
    "            )\n",
    "            X_original[:, i] = restored.astype(X_remapped.dtype, copy=False)\n",
    "\n",
    "        return X_original\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c08d9c-2b44-442d-874e-e1f619dbf9ae",
   "metadata": {},
   "source": [
    "# Torch Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df335f82-466a-4794-8524-a912a2a77c09",
   "metadata": {},
   "source": [
    "The **`TorchLinearRegression`** class implements a GPU/CPU-flexible linear regression solver in PyTorch with optional CuPy interoperability for seamless use in GPU-based workflows. The constructor allows configuration of whether to fit an intercept, target device (`cpu` or `cuda`), numeric precision (`dtype`), and solver strategy (`lstsq` for robustness or `normal` equations for speed). Internally, helper methods safely convert between CuPy and Torch tensors via DLPack while preserving data ownership. The `fit` method constructs the design matrix (adding a bias column if requested), solves for coefficients using the specified method, and stores both Torch tensors and CuPy-accessible versions of weights and intercepts. The `predict` method generates outputs from new data, while `score` computes the coefficient of determination (R²) to evaluate model performance. A specialized `_fast_cv_score` method performs k-fold cross-validation entirely in Torch to avoid DLPack hazards, ensuring safe and efficient GPU computations. Overall, this design provides a flexible and high-performance linear regression class, bridging PyTorch’s optimization routines with CuPy integration for workflows that demand both speed and compatibility with GPU array libraries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9767cebc-d660-4a0d-9fed-c92c73681f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchLinearRegression:\n",
    "    def __init__(\n",
    "        self,\n",
    "        fit_intercept: bool = True,\n",
    "        device: Optional[str] = None,   # None -> 'cuda' if available else 'cpu'\n",
    "        dtype: torch.dtype = torch.float32,\n",
    "        solver: str = \"lstsq\",          # 'lstsq' (robust) or 'normal'\n",
    "    ):\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "        self.solver = solver\n",
    "\n",
    "        # learned params exposed as CuPy for your workflow\n",
    "        self.coef_: Optional[\"cp.ndarray\"] = None          # (n_features,) or (n_targets, n_features)\n",
    "        self.intercept_: Optional[\"cp.ndarray\"] = None     # () or (n_targets,)\n",
    "\n",
    "        # internal torch copies\n",
    "        self._coef_t: Optional[torch.Tensor] = None        # (n_targets, n_features)\n",
    "        self._intercept_t: Optional[torch.Tensor] = None   # (n_targets,)\n",
    "\n",
    "    # -------- helpers --------\n",
    "    def _infer_device(self) -> str:\n",
    "        if self.device is not None:\n",
    "            return self.device\n",
    "        return \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    @staticmethod\n",
    "    def _is_cupy(x: Any) -> bool:\n",
    "        return _HAS_CUPY and (type(x).__module__.split(\".\")[0] == \"cupy\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _torch_to_cupy(t: torch.Tensor) -> \"cp.ndarray\":\n",
    "        if not _HAS_CUPY:\n",
    "            raise RuntimeError(\"CuPy is required for cp.ndarray outputs.\")\n",
    "        if t.device.type != \"cuda\":\n",
    "            if not torch.cuda.is_available():\n",
    "                raise RuntimeError(\"CUDA not available for cp.ndarray output.\")\n",
    "            t = t.to(\"cuda\")\n",
    "        torch.cuda.synchronize()\n",
    "        from_dl = getattr(cp, \"from_dlpack\", None) or getattr(cp, \"fromDlpack\", None)\n",
    "        return from_dl(torch_dlpack.to_dlpack(t.detach()))\n",
    "\n",
    "    @staticmethod\n",
    "    def _cupy_to_torch_owning(x_cp_src: \"cp.ndarray\", device: str, dtype: torch.dtype) -> torch.Tensor:\n",
    "        # make a private, contiguous GPU copy so the original CuPy array remains safe after DLPack handoff\n",
    "        x_cp = cp.ascontiguousarray(x_cp_src)\n",
    "        cp.cuda.get_current_stream().synchronize()\n",
    "        to_dl = getattr(x_cp, \"toDlpack\", None) or getattr(x_cp, \"to_dlpack\", None)\n",
    "        t = torch_dlpack.from_dlpack(to_dl())\n",
    "        if device:\n",
    "            t = t.to(device)\n",
    "        if dtype:\n",
    "            t = t.to(dtype)\n",
    "        return t\n",
    "\n",
    "    def _to_tensor(self, x: _Array, device: str, dtype: torch.dtype) -> torch.Tensor:\n",
    "        if torch.is_tensor(x):\n",
    "            return x.to(device=device, dtype=dtype)\n",
    "        if self._is_cupy(x):\n",
    "            return self._cupy_to_torch_owning(x, device, dtype)\n",
    "        raise TypeError(f\"Unsupported array type: {type(x)}\")\n",
    "\n",
    "    def _from_tensor(self, t: torch.Tensor) -> \"cp.ndarray\":\n",
    "        return self._torch_to_cupy(t)\n",
    "\n",
    "    # -------- core API --------\n",
    "    def get_params(self, deep: bool = True) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"fit_intercept\": self.fit_intercept,\n",
    "            \"device\": self.device,\n",
    "            \"dtype\": self.dtype,\n",
    "            \"solver\": self.solver,\n",
    "        }\n",
    "\n",
    "    def set_params(self, **params) -> \"TorchLinearRegression\":\n",
    "        for k, v in params.items():\n",
    "            setattr(self, k, v)\n",
    "        return self\n",
    "\n",
    "    def fit(self, X: _Array, y: _Array) -> \"TorchLinearRegression\":\n",
    "        dev = self._infer_device()\n",
    "        X_t = self._to_tensor(X, dev, self.dtype)\n",
    "        y_t = self._to_tensor(y, dev, self.dtype)\n",
    "\n",
    "        if y_t.ndim == 1:\n",
    "            y_t = y_t.unsqueeze(1)\n",
    "\n",
    "        if self.fit_intercept:\n",
    "            ones = torch.ones((X_t.shape[0], 1), device=dev, dtype=self.dtype)\n",
    "            X_aug = torch.cat([X_t, ones], dim=1)\n",
    "        else:\n",
    "            X_aug = X_t\n",
    "\n",
    "        if self.solver == \"normal\":\n",
    "            XtX = X_aug.T @ X_aug\n",
    "            XtX = XtX + 1e-8 * torch.eye(XtX.shape[0], device=dev, dtype=self.dtype)\n",
    "            Xty = X_aug.T @ y_t\n",
    "            beta = torch.linalg.solve(XtX, Xty)\n",
    "        else:\n",
    "            beta = torch.linalg.lstsq(X_aug, y_t).solution\n",
    "\n",
    "        if self.fit_intercept:\n",
    "            w = beta[:-1, :]       # (n_features, n_targets)\n",
    "            b = beta[-1, :]        # (n_targets,)\n",
    "        else:\n",
    "            w = beta\n",
    "            b = torch.zeros((y_t.shape[1],), device=dev, dtype=self.dtype)\n",
    "\n",
    "        self._coef_t = w.T.contiguous()      # (n_targets, n_features)\n",
    "        self._intercept_t = b.contiguous()   # (n_targets,)\n",
    "\n",
    "        # expose CuPy params\n",
    "        if y_t.shape[1] == 1:\n",
    "            self.coef_ = self._from_tensor(self._coef_t)[0]            # (n_features,)\n",
    "            self.intercept_ = self._from_tensor(self._intercept_t)[0]  # 0-d cp array\n",
    "        else:\n",
    "            self.coef_ = self._from_tensor(self._coef_t)               # (n_targets, n_features)\n",
    "            self.intercept_ = self._from_tensor(self._intercept_t)     # (n_targets,)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X: _Array) -> \"cp.ndarray\":\n",
    "        if self._coef_t is None or self._intercept_t is None:\n",
    "            raise RuntimeError(\"Model is not fitted.\")\n",
    "        dev = self._coef_t.device.type\n",
    "        X_t = self._to_tensor(X, dev, self._coef_t.dtype)\n",
    "        y = X_t @ self._coef_t.T\n",
    "        y = y + (self._intercept_t if self._intercept_t.ndim != 0 else self._intercept_t)\n",
    "        return self._from_tensor(y.squeeze(-1))\n",
    "\n",
    "    def score(self, X: _Array, y: _Array) -> float:\n",
    "        if self._coef_t is None:\n",
    "            raise RuntimeError(\"Model is not fitted.\")\n",
    "        dev = self._infer_device()\n",
    "        X_t = self._to_tensor(X, dev, self._coef_t.dtype)\n",
    "        y_t = self._to_tensor(y, dev, self._coef_t.dtype)\n",
    "        if y_t.ndim == 1:\n",
    "            y_t = y_t.unsqueeze(1)\n",
    "\n",
    "        y_hat = X_t @ self._coef_t.T\n",
    "        y_hat = y_hat + (self._intercept_t if self._intercept_t.ndim != 0 else self._intercept_t)\n",
    "\n",
    "        ss_res = torch.sum((y_t - y_hat) ** 2, dim=0)\n",
    "        y_mean = torch.mean(y_t, dim=0, keepdim=True)\n",
    "        ss_tot = torch.sum((y_t - y_mean) ** 2, dim=0).clamp_min(1e-12)\n",
    "        r2 = 1.0 - (ss_res / ss_tot)\n",
    "        return float(torch.mean(r2).item())\n",
    "\n",
    "    # -------- CV stays in Torch to avoid DLPack hazards --------\n",
    "        def _cv_r2_gpu(self, X_cp, y_cp, k=3):\n",
    "            return _fast_cv_score(X_cp, y_cp, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2782f8-ddd7-4a8a-b2f1-d441ca8de80b",
   "metadata": {},
   "source": [
    "# Additive Polynomial Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b3c768-3a4d-4b9b-8bf6-c4e853df07fc",
   "metadata": {},
   "source": [
    "The **`VectorizedAPS`** class implements a GPU-accelerated version of *Additive Polynomial Selection (APS)* for nonlinear feature discovery and regression modeling. It combines **CuPy** for array-level GPU operations with **TorchLinearRegression** for efficient linear regression fits, making it well-suited for large-scale, high-dimensional datasets.\n",
    "\n",
    "During the `fit` process, input data is first moved to the GPU and then expanded into polynomial features up to the specified degree. Candidate features are filtered according to interaction order, allowing restrictions on the number of variables included in each monomial. A greedy, vectorized forward-selection algorithm is then applied, evaluating candidates in parallel with GPU-backed cross-validation R² scores to identify the most informative polynomial terms. The final model is trained using **TorchLinearRegression** on the selected features, with a fallback to raw inputs if no candidates meet the tolerance threshold.\n",
    "\n",
    "The `predict` method ensures consistency by regenerating polynomial features at inference and applying the same selected indices. Helper methods provide additional functionality such as filtering by interaction complexity, efficient candidate evaluation, and fast GPU-based cross-validation that avoids costly CPU–GPU transfers. A static method `_gpu_poly_features` constructs polynomial features and generates human-readable feature names directly in GPU memory, maintaining speed and interpretability.\n",
    "\n",
    "Overall, the design of **`VectorizedAPS`** balances interpretability, efficiency, and scalability. It enables effective feature engineering and polynomial model selection in GPU workflows, making it a powerful tool for uncovering nonlinear relationships in high-dimensional data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d995885e-5e18-4c2c-9f68-d8c175dd3e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorizedAPS:\n",
    "    \"\"\"Vectorized Additive Polynomial Selection (GPU/CuPy + Torch OLS)\"\"\"\n",
    "\n",
    "    def __init__(self, max_degree=3, max_interaction_order=2, tolerance=1e-4, verbose=False):\n",
    "        self.max_degree = int(max_degree)\n",
    "        self.max_interaction_order = int(max_interaction_order)\n",
    "        self.tolerance = float(tolerance)\n",
    "        self.verbose = verbose\n",
    "        self.selected_features = None           # indices within candidates\n",
    "        self.final_model = None                 # TorchLinearRegression\n",
    "        self._feature_names_all = None          # all poly feature names\n",
    "        self._candidate_indices = None          # filtered indices (interaction <= K)\n",
    "\n",
    "    # ---------- public API ----------\n",
    "    def fit(self, X, y):\n",
    "        # move to GPU \n",
    "        X_cp = cp.asarray(X)\n",
    "        y_cp = cp.asarray(y).reshape(-1)\n",
    "\n",
    "        # build polynomial features on GPU\n",
    "        X_poly, feat_names = self._gpu_poly_features(X_cp, self.max_degree, include_bias=False)\n",
    "        self._feature_names_all = feat_names\n",
    "\n",
    "        # filter by max interaction order\n",
    "        candidate_indices = self._filter_by_interaction_order(feat_names)\n",
    "        if len(candidate_indices) == 0:\n",
    "            candidate_indices = list(range(min(X_cp.shape[1], X_poly.shape[1])))\n",
    "\n",
    "        X_cand = X_poly[:, candidate_indices]\n",
    "        self._candidate_indices = candidate_indices\n",
    "\n",
    "        # forward selection (GPU-backed CV)\n",
    "        self.selected_features = self._vectorized_forward_selection(X_cand, y_cp)\n",
    "\n",
    "        # fit final model on selected poly features (or fallback to raw X)\n",
    "        from math import inf\n",
    "        if self.selected_features and len(self.selected_features) > 0:\n",
    "            X_final = X_cand[:, self.selected_features]\n",
    "        else:\n",
    "            # fallback — use raw X if nothing selected\n",
    "            X_final = X_cp\n",
    "\n",
    "        lr = TorchLinearRegression(fit_intercept=True, device=\"cuda\")\n",
    "        lr.fit(X_final, y_cp)\n",
    "        self.final_model = lr\n",
    "\n",
    "        # store absolute poly indices for prediction-time selection\n",
    "        if self.selected_features and len(self.selected_features) > 0:\n",
    "            abs_idxs = [candidate_indices[i] for i in self.selected_features]\n",
    "        else:\n",
    "            abs_idxs = []\n",
    "        # piggyback “selected_features” like your original did\n",
    "        self.final_model.selected_features = abs_idxs\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.final_model is None:\n",
    "            return cp.zeros(len(X), dtype=cp.float32)   # or cp.float64 if you prefer\n",
    "\n",
    "        X_cp = cp.asarray(X)\n",
    "\n",
    "        # rebuild poly features and select the same columns\n",
    "        if hasattr(self.final_model, \"selected_features\") and len(self.final_model.selected_features) > 0:\n",
    "            X_poly, _ = self._gpu_poly_features(X_cp, self.max_degree, include_bias=False)\n",
    "            abs_idxs = self.final_model.selected_features\n",
    "            if len(abs_idxs) > 0 and max(abs_idxs) < X_poly.shape[1]:\n",
    "                X_selected = X_poly[:, abs_idxs]\n",
    "                return self.final_model.predict(X_selected)  # returns NumPy\n",
    "        # fallback: predict on raw X\n",
    "        return self.final_model.predict(X_cp)\n",
    "\n",
    "    # ---------- helpers ----------\n",
    "    def _filter_by_interaction_order(self, feature_names):\n",
    "        \"\"\"Keep features whose monomial uses ≤ max_interaction_order unique variables.\"\"\"\n",
    "        valid = []\n",
    "        for i, name in enumerate(feature_names):\n",
    "            # names look like: 'x0', 'x1^2', 'x0 x1', 'x0^2 x2', etc.\n",
    "            var_matches = re.findall(r'x(\\d+)', name)\n",
    "            if len(set(var_matches)) <= self.max_interaction_order:\n",
    "                valid.append(i)\n",
    "        return valid\n",
    "\n",
    "    def _vectorized_forward_selection(self, X_cand, y_cp):\n",
    "        return _vectorized_forward_selection(X_cand, y_cp, self.tolerance, self.verbose)\n",
    "\n",
    "    def _batch_evaluate_candidates(self, X_cand, y_cp, selected, remaining):\n",
    "        \"\"\"Evaluate a batch of candidates by k-fold R², stays on GPU for data/fit.\"\"\"\n",
    "        scores = []\n",
    "        X_sel = X_cand[:, selected] if selected else None\n",
    "\n",
    "        for cand in remaining:\n",
    "            # assemble [selected | candidate]\n",
    "            X_test = X_cand[:, [cand]] if X_sel is None else cp.concatenate([X_sel, X_cand[:, [cand]]], axis=1)\n",
    "            score = self._fast_cv_score(X_test, y_cp, cv_folds=3)\n",
    "            scores.append(score)\n",
    "\n",
    "        return cp.asarray(scores, dtype=cp.float32)\n",
    "\n",
    "    def _fast_cv_score(self, X_cp, y_cp, cv_folds=3):\n",
    "        return _fast_cv_score(X_cp, y_cp, cv_folds)\n",
    "\n",
    "    # ---------- GPU polynomial features ----------\n",
    "    @staticmethod\n",
    "    def _gpu_poly_features(X_cp: cp.ndarray, degree: int, include_bias: bool = False):\n",
    "        return gpu_poly_features(X_cp, degree, include_bias)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94aa2495-ed28-4ccf-a024-03a9c805b780",
   "metadata": {},
   "source": [
    "# Multiplicative Polynomial Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4573167-1518-449e-ad4f-3e43e521674c",
   "metadata": {},
   "source": [
    "The **`VectorizedMAPS`** class implements a GPU-accelerated approach to *Multiplicative Polynomial Selection (MAPS)* for modeling nonlinear relationships in data. Its workflow emphasizes interpretability and stability by combining quantile remapping, log-space transformations, and forward feature selection. In the `fit` stage, input data is transferred to GPU memory using CuPy, and the target variable is shifted into the positive domain (if needed) to allow log transformation. Explanatory variables are quantile-remapped for consistency and interpretability, then expanded into multiplicative polynomial features up to a chosen degree. Candidate terms are filtered by their interaction order (e.g., pairwise vs. higher-order interactions), after which a greedy, vectorized forward-selection routine identifies the subset of features that improve cross-validated log-space R² scores. The final model is fit using a GPU-backed **TorchLinearRegression**, and selected polynomial indices are stored for use at inference. In `predict`, new inputs are remapped, polynomial features rebuilt, and log-space predictions are exponentiated and shifted back to the original scale. Helper methods handle feature filtering, batched candidate evaluation, and fast GPU-based cross-validation, while `_gpu_poly_features` constructs polynomial expansions with readable names. Together, this class provides a scalable, interpretable, and high-performance method for multiplicative polynomial modeling entirely on the GPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ec9b168-7c42-4107-8ff1-e9b0834250eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorizedMAPS:\n",
    "    \"\"\"Vectorized Multiplicative Polynomial Selection (GPU: CuPy + Torch OLS)\"\"\"\n",
    "\n",
    "    def __init__(self, max_degree=3, max_interaction_order=2, tolerance=1e-4, verbose=False):\n",
    "        self.max_degree = int(max_degree)\n",
    "        self.max_interaction_order = int(max_interaction_order)\n",
    "        self.tolerance = float(tolerance)\n",
    "        self.verbose = verbose\n",
    "        self.selected_features = None\n",
    "        self.final_model = None\n",
    "        self.location_shift = 0.0\n",
    "        self.quantile_remapper = None\n",
    "        self._feature_names_all = None\n",
    "        self._candidate_indices = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # move to GPU\n",
    "        X_cp = cp.asarray(X)\n",
    "        y_cp = cp.asarray(y).reshape(-1)\n",
    "\n",
    "        # ensure positivity -> log space\n",
    "        y_min = float(cp.min(y_cp))\n",
    "        if y_min <= 0.0:\n",
    "            self.location_shift = abs(y_min) + 1.0\n",
    "            y_pos = y_cp + self.location_shift\n",
    "        else:\n",
    "            self.location_shift = 0.0\n",
    "            y_pos = y_cp\n",
    "        log_y_cp = cp.log(y_pos)\n",
    "\n",
    "        # quantile remap (for interpretability) on GPU\n",
    "        self.quantile_remapper = QuantileRemapper().fit(X_cp)\n",
    "        X_remapped = self.quantile_remapper.transform(X_cp)\n",
    "\n",
    "        # polynomial features on remapped variables (GPU)\n",
    "        X_poly, feat_names = self._gpu_poly_features(X_remapped, self.max_degree, include_bias=False)\n",
    "        self._feature_names_all = feat_names\n",
    "\n",
    "        # filter by interaction order\n",
    "        candidate_indices = self._filter_by_interaction_order(feat_names)\n",
    "        if len(candidate_indices) == 0:\n",
    "            candidate_indices = list(range(min(X_cp.shape[1], X_poly.shape[1])))\n",
    "        self._candidate_indices = candidate_indices\n",
    "\n",
    "        X_cand = X_poly[:, candidate_indices]\n",
    "\n",
    "        # forward selection in log space\n",
    "        self.selected_features = self._vectorized_forward_selection(X_cand, log_y_cp)\n",
    "\n",
    "        # final fit in log space (selected poly -> log_y)\n",
    "        if self.selected_features and len(self.selected_features) > 0:\n",
    "            X_final = X_cand[:, self.selected_features]\n",
    "        else:\n",
    "            # fallback: use remapped main effects if nothing selected\n",
    "            X_final = X_remapped\n",
    "\n",
    "        lr = TorchLinearRegression(fit_intercept=True, device=\"cuda\")\n",
    "        lr.fit(X_final, log_y_cp)\n",
    "        self.final_model = lr\n",
    "\n",
    "        # store absolute poly indices for prediction-time selection\n",
    "        if self.selected_features and len(self.selected_features) > 0:\n",
    "            abs_idxs = [candidate_indices[i] for i in self.selected_features]\n",
    "        else:\n",
    "            abs_idxs = []\n",
    "        self.final_model.selected_features = abs_idxs\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.final_model is None:\n",
    "            return cp.zeros(len(X), dtype=float)\n",
    "\n",
    "        X_cp = cp.asarray(X)\n",
    "\n",
    "        # remap using fitted quantile mapper (fallback: identity)\n",
    "        if self.quantile_remapper is None:\n",
    "            X_remapped = X_cp\n",
    "        else:\n",
    "            X_remapped = self.quantile_remapper.transform(X_cp)\n",
    "\n",
    "        # predict in log space, then invert\n",
    "        if hasattr(self.final_model, \"selected_features\") and len(self.final_model.selected_features) > 0:\n",
    "            X_poly, _ = self._gpu_poly_features(X_remapped, self.max_degree, include_bias=False)\n",
    "            abs_idxs = self.final_model.selected_features\n",
    "            if len(abs_idxs) > 0 and max(abs_idxs) < X_poly.shape[1]:\n",
    "                X_sel = X_poly[:, abs_idxs]\n",
    "                log_y_pred = self.final_model.predict(X_sel)     # NumPy\n",
    "            else:\n",
    "                log_y_pred = self.final_model.predict(X_remapped)\n",
    "        else:\n",
    "            log_y_pred = self.final_model.predict(X_remapped)\n",
    "\n",
    "        y_pred = cp.exp(log_y_pred) - self.location_shift\n",
    "        return y_pred\n",
    "\n",
    "    # ------------ helpers ------------\n",
    "    def _filter_by_interaction_order(self, feature_names):\n",
    "        valid = []\n",
    "        for i, name in enumerate(feature_names):\n",
    "            var_matches = re.findall(r'x(\\d+)', name)\n",
    "            if len(set(var_matches)) <= self.max_interaction_order:\n",
    "                valid.append(i)\n",
    "        return valid\n",
    "\n",
    "    def _vectorized_forward_selection(self, X_cand, y_cp):\n",
    "        return _vectorized_forward_selection(X_cand, y_cp, self.tolerance, self.verbose)\n",
    "\n",
    "    def _batch_evaluate_candidates(self, X_cand, log_y_cp, selected, remaining):\n",
    "        scores = []\n",
    "        X_sel = X_cand[:, selected] if selected else None\n",
    "        for cand in remaining:\n",
    "            X_test = X_cand[:, [cand]] if X_sel is None else cp.concatenate([X_sel, X_cand[:, [cand]]], axis=1)\n",
    "            scores.append(self._fast_cv_score(X_test, log_y_cp, cv_folds=3))\n",
    "        return cp.asarray(scores, dtype=float)\n",
    "\n",
    "    def _cv_r2_gpu(self, X_cp, y_cp, k=3):\n",
    "        return _fast_cv_score(X_cp, y_cp, k)\n",
    "\n",
    "    @staticmethod\n",
    "    def _gpu_poly_features(X_cp: cp.ndarray, degree: int, include_bias: bool = False):\n",
    "        return gpu_poly_features(X_cp, degree, include_bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcafd33-2acc-4847-a7b9-e81877b2d1d7",
   "metadata": {},
   "source": [
    "# Adaptive Method Selector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39ccf02-e779-4dc9-ae7c-f5c0251deab0",
   "metadata": {},
   "source": [
    "The **`VectorizedAdaptiveMethodSelector`** (AMS) orchestrates an automated, GPU-accelerated choice between **APS** (additive polynomial selection) and **MAPS** (multiplicative polynomial selection) to fit the most suitable model for a given dataset. In `fit`, it first runs a fast, CuPy-backed characteristic analysis that scores the data on four axes: **multiplicative evidence** (whether log-space linearization helps), **interaction strength** (how much higher-degree terms improve cross-validated R² over main effects), **multiplicative separability** (how well a sum of univariate log models explains the response compared with a joint model), and **dynamic range** of `y`. A transparent decision function then chooses APS, MAPS, or a brief **both-methods** trial if the evidence is ambiguous; the chosen path trains a GPU-native polynomial model using `TorchLinearRegression` under the hood and records the selected features for reproducible inference. The class exposes `predict` for seamless forecasting via the selected submodel, `get_metrics` for quick reporting, and helper routines (`_cv_r2_gpu`, `_gpu_poly_features`, and the three diagnostic tests) that keep all heavy lifting on the GPU to minimize transfers. In short, AMS is a vectorized controller that profiles your problem, selects the right polynomial paradigm, and delivers a high-performance fit with concise, interpretable signals about why that choice was made.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2796f7b-7810-49b8-940a-0e77f7977bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorizedAdaptiveMethodSelector:\n",
    "    \"\"\"Adaptive Method Selector with GPU batch processing (CuPy + Torch OLS).\"\"\"\n",
    "\n",
    "    def __init__(self, max_degree=3, max_interaction_order=2, tolerance=1e-4, verbose=False):\n",
    "        self.max_degree = int(max_degree)\n",
    "        self.max_interaction_order = int(max_interaction_order)\n",
    "        self.tolerance = float(tolerance)\n",
    "        self.verbose = verbose\n",
    "        self.selected_method = None\n",
    "        self.method_scores = {}\n",
    "        self.final_model = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if self.verbose:\n",
    "            print(\"=== Vectorized AMS Analysis (GPU) ===\")\n",
    "\n",
    "        self.method_scores = self._vectorized_analyze_characteristics(X, y)\n",
    "        recommended_method, confidence = self._make_method_decision(self.method_scores)\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"Recommended: {recommended_method} (confidence: {confidence:.3f})\")\n",
    "\n",
    "        if recommended_method == 'APS':\n",
    "            self.selected_method = 'APS'\n",
    "            self.final_model = VectorizedAPS(self.max_degree, self.max_interaction_order, self.tolerance, False)\n",
    "            self.final_model.fit(X, y)\n",
    "\n",
    "        elif recommended_method == 'MAPS':\n",
    "            self.selected_method = 'MAPS'\n",
    "            self.final_model = VectorizedMAPS(self.max_degree, self.max_interaction_order, self.tolerance, False)\n",
    "            self.final_model.fit(X, y)\n",
    "\n",
    "        else:  # BOTH_TEST\n",
    "            if self.verbose:\n",
    "                print(\"Testing both methods (GPU)...\")\n",
    "            aps_model = VectorizedAPS(self.max_degree, self.max_interaction_order, self.tolerance, False)\n",
    "            aps_model.fit(X, y)\n",
    "            aps_pred = aps_model.predict(X)\n",
    "            aps_score = self._r2_cp(cp.asarray(y), cp.asarray(aps_pred))\n",
    "\n",
    "            maps_model = VectorizedMAPS(self.max_degree, self.max_interaction_order, self.tolerance, False)\n",
    "            maps_model.fit(X, y)\n",
    "            maps_pred = maps_model.predict(X)\n",
    "            maps_score = self._r2_cp(cp.asarray(y), cp.asarray(maps_pred))\n",
    "\n",
    "            if self.verbose:\n",
    "                print(f\"APS R²: {aps_score:.4f}, MAPS R²: {maps_score:.4f}\")\n",
    "\n",
    "            if aps_score > maps_score:\n",
    "                self.selected_method = 'APS'\n",
    "                self.final_model = aps_model\n",
    "            else:\n",
    "                self.selected_method = 'MAPS'\n",
    "                self.final_model = maps_model\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.final_model is None:\n",
    "            raise ValueError(\"Model must be fitted first\")\n",
    "        return self.final_model.predict(X)\n",
    "\n",
    "    # ---------------- GPU characteristic analysis ----------------\n",
    "\n",
    "    def _vectorized_analyze_characteristics(self, X, y):\n",
    "        X_cp = cp.asarray(X)\n",
    "        y_cp = cp.asarray(y).reshape(-1)\n",
    "\n",
    "        scores = {}\n",
    "        scores['multiplicative_evidence']   = self._vectorized_multiplicativity_test(X_cp, y_cp)\n",
    "        scores['interaction_strength']      = self._vectorized_interaction_test(X_cp, y_cp)\n",
    "        scores['multiplicative_separability']= self._vectorized_separability_test(X_cp, y_cp)\n",
    "        ymax = float(cp.max(y_cp))\n",
    "        ymin = float(cp.min(y_cp))\n",
    "        scores['dynamic_range'] = ymax / max(ymin, 1e-8)\n",
    "        return scores\n",
    "\n",
    "    def _vectorized_multiplicativity_test(self, X_cp, y_cp):\n",
    "        try:\n",
    "            n = int(y_cp.size)\n",
    "            if n < 20:\n",
    "                return 0.5\n",
    "\n",
    "            ymax = float(cp.max(y_cp))\n",
    "            ymin = float(cp.min(y_cp))\n",
    "            dynamic_range = ymax / max(ymin, 1e-8)\n",
    "            if dynamic_range < 3.0:\n",
    "                return 0.0\n",
    "\n",
    "            n_bootstrap = 5\n",
    "            n_samples = min(n, max(50, n // 2))\n",
    "\n",
    "            rng = cp.random.default_rng(42)\n",
    "            improvement_ratios = []\n",
    "\n",
    "            for _ in range(n_bootstrap):\n",
    "                idx = rng.integers(0, n, size=n_samples, endpoint=False)\n",
    "                idx_cp = cp.asarray(idx)\n",
    "                Xb = X_cp[idx_cp]\n",
    "                yb = y_cp[idx_cp]\n",
    "\n",
    "                # Original space OLS\n",
    "                lr1 = TorchLinearRegression(fit_intercept=True, device=\"cuda\")\n",
    "                lr1.fit(Xb, yb)\n",
    "                yhat1 = lr1.predict(Xb)  # numpy\n",
    "                yb_cp = cp.asnumpy(yb)\n",
    "                res1 = cp.mean((yb_cp - yhat1)**2)\n",
    "\n",
    "                # Log space (only if all positive)\n",
    "                if float(cp.min(yb)) > 0.0:\n",
    "                    log_yb = cp.log(yb)\n",
    "                    lr2 = TorchLinearRegression(fit_intercept=True, device=\"cuda\")\n",
    "                    lr2.fit(Xb, log_yb)\n",
    "                    log_pred = lr2.predict(Xb)          # numpy\n",
    "                    yhat2 = cp.exp(log_pred)\n",
    "                    res2 = cp.mean((yb_cp - yhat2)**2)\n",
    "                    ratio = res1 / max(res2, 1e-8)\n",
    "                else:\n",
    "                    ratio = 0.5\n",
    "\n",
    "                improvement_ratios.append(ratio)\n",
    "\n",
    "            ratios = cp.asarray(improvement_ratios, dtype=float)\n",
    "            med = float(cp.median(ratios))\n",
    "            std = float(cp.std(ratios))\n",
    "            if std > 0.5:\n",
    "                med *= 0.8\n",
    "\n",
    "            if   med > 3.0: mult = 0.9\n",
    "            elif med > 2.0: mult = 0.8\n",
    "            elif med > 1.5: mult = 0.6\n",
    "            elif med < 0.6: mult = 0.1\n",
    "            elif med < 0.8: mult = 0.3\n",
    "            else:           mult = 0.4\n",
    "\n",
    "            if dynamic_range > 20:\n",
    "                mult = min(1.0, mult + 0.15)\n",
    "            elif dynamic_range > 10:\n",
    "                mult = min(1.0, mult + 0.10)\n",
    "            return mult\n",
    "\n",
    "        except Exception:\n",
    "            return 0.5\n",
    "\n",
    "    def _vectorized_interaction_test(self, X_cp, y_cp):\n",
    "        try:\n",
    "            # degree 1 (main effects) vs full poly up to max_degree\n",
    "            X_main, _ = self._gpu_poly_features(X_cp, degree=1, include_bias=False)\n",
    "            X_full, _ = self._gpu_poly_features(X_cp, degree=self.max_degree, include_bias=False)\n",
    "\n",
    "            r2_main = self._cv_r2_gpu(X_main, y_cp, k=min(5, max(2, int(y_cp.size)//10)))\n",
    "            r2_full = self._cv_r2_gpu(X_full, y_cp, k=min(5, max(2, int(y_cp.size)//10)))\n",
    "\n",
    "            r2_main = max(0.0, r2_main)\n",
    "            r2_full = max(0.0, r2_full)\n",
    "            if r2_full <= r2_main:\n",
    "                return 0.0\n",
    "\n",
    "            return float(min(1.0, (r2_full - r2_main) / max(1.0 - r2_main, 1e-8)))\n",
    "        except Exception:\n",
    "            return 0.5\n",
    "\n",
    "    def _vectorized_separability_test(self, X_cp, y_cp):\n",
    "        try:\n",
    "            ymin = float(cp.min(y_cp))\n",
    "            if ymin <= 0.0:\n",
    "                y_pos = y_cp - ymin + 1.0\n",
    "            else:\n",
    "                y_pos = y_cp\n",
    "            log_y = cp.log(y_pos)\n",
    "\n",
    "            d = int(X_cp.shape[1])\n",
    "            if d < 2:\n",
    "                return 0.0\n",
    "\n",
    "            # Univariate fits (degree up to 2)\n",
    "            preds = []\n",
    "            for j in range(d):\n",
    "                Xj = X_cp[:, [j]]\n",
    "                Xj_poly, _ = self._gpu_poly_features(Xj, degree=min(2, self.max_degree), include_bias=False)\n",
    "                lr = TorchLinearRegression(fit_intercept=True, device=\"cuda\")\n",
    "                lr.fit(Xj_poly, log_y)\n",
    "                pj = lr.predict(Xj_poly)  # numpy\n",
    "                preds.append(pj)\n",
    "\n",
    "            sum_uni = cp.sum(preds, axis=0)\n",
    "            # center to avoid overcounting intercepts\n",
    "            sum_uni = sum_uni - (d - 1) * float(cp.mean(cp.asnumpy(log_y)))\n",
    "            r2_sep = self._r2_cp(cp.asnumpy(log_y), sum_uni)\n",
    "\n",
    "            # Full multivariate log fit up to degree 2 (to be conservative)\n",
    "            X_full, _ = self._gpu_poly_features(X_cp, degree=min(2, self.max_degree), include_bias=False)\n",
    "            lr_full = TorchLinearRegression(fit_intercept=True, device=\"cuda\")\n",
    "            lr_full.fit(X_full, log_y)\n",
    "            pred_full = lr_full.predict(X_full)\n",
    "            r2_full = self._r2_cp(cp.asnumpy(log_y), pred_full)\n",
    "\n",
    "            if r2_full <= 0.1:\n",
    "                return 0.0\n",
    "            return float(min(1.0, r2_sep / max(r2_full, 1e-8)))\n",
    "        except Exception:\n",
    "            return 0.5\n",
    "\n",
    "    # ---------------- decision & metrics ----------------\n",
    "\n",
    "    def _make_method_decision(self, scores):\n",
    "        mult_evidence = scores['multiplicative_evidence']\n",
    "        interaction   = scores['interaction_strength']\n",
    "        separability  = scores['multiplicative_separability']\n",
    "        dynamic_range = scores['dynamic_range']\n",
    "        \n",
    "        # More reasonable thresholds for MAPS selection\n",
    "        if mult_evidence > 0.55:  # Lowered from 0.7\n",
    "            return 'MAPS', mult_evidence\n",
    "        \n",
    "        if mult_evidence < 0.25:  # Lowered from 0.35\n",
    "            return 'APS', 1 - mult_evidence\n",
    "        \n",
    "        # Consider dynamic range and separability with lower thresholds\n",
    "        if dynamic_range > 10 and separability > 0.75:  # Much more reasonable\n",
    "            return 'MAPS', separability * 0.8\n",
    "        \n",
    "        # Strong separability indicator\n",
    "        if separability > 0.85 and dynamic_range > 5:  # Lowered thresholds\n",
    "            return 'MAPS', separability\n",
    "        \n",
    "        # Multiplicative evidence in middle range\n",
    "        if mult_evidence > 0.4:  # Give MAPS more consideration\n",
    "            if separability > 0.7 or dynamic_range > 8:\n",
    "                return 'MAPS', mult_evidence\n",
    "        \n",
    "        # Low interaction strength suggests simpler model\n",
    "        if interaction < 0.3:\n",
    "            return 'APS', 1 - interaction\n",
    "        \n",
    "        # Default to testing both methods for borderline cases\n",
    "        return 'BOTH_TEST', 0.5\n",
    "\n",
    "    def get_metrics(self):\n",
    "        if self.final_model is None:\n",
    "            return {\"method\": \"VectorizedAMS\", \"n_features\": 0, \"r2\": 0, \"aic\": 0, \"score\": 0}\n",
    "        n_features = len(getattr(self.final_model, 'selected_features', []))\n",
    "        return {\"method\": f\"VectorizedAMS->{self.selected_method}\", \"n_features\": n_features, \"r2\": 0, \"aic\": 0, \"score\": 0}\n",
    "\n",
    "    # ---------------- small helpers ----------------\n",
    "\n",
    "    def _cv_r2_gpu(self, X_cp, y_cp, k=3):\n",
    "        n = int(X_cp.shape[0])\n",
    "        if int(X_cp.shape[1]) >= 0.8*n or n < 15:\n",
    "            return -10.0\n",
    "    \n",
    "        # Fix: use k parameter instead of undefined cv_folds\n",
    "        k = int(min(k, max(2, n // 10)))\n",
    "        \n",
    "        # simple KFold indices\n",
    "        rng = cp.random.default_rng(42)\n",
    "        idx = rng.permutation(n)       # cp.ndarray of [0..n-1] permuted on GPU\n",
    "        folds = cp.array_split(idx, k)\n",
    "    \n",
    "        r2s = []\n",
    "        for i in range(k):\n",
    "            val_idx = folds[i]\n",
    "            trn_idx = cp.concatenate([folds[j] for j in range(k) if j != i])\n",
    "            X_tr = X_cp[cp.asarray(trn_idx)]\n",
    "            y_tr = y_cp[cp.asarray(trn_idx)]\n",
    "            X_va = X_cp[cp.asarray(val_idx)]\n",
    "            y_va = y_cp[cp.asarray(val_idx)]\n",
    "    \n",
    "            lr = TorchLinearRegression(fit_intercept=True, device=\"cuda\")\n",
    "            lr.fit(X_tr, y_tr)\n",
    "            y_hat = lr.predict(X_va)      # numpy\n",
    "            r2 = self._r2_cp(cp.asnumpy(y_va), y_hat)\n",
    "            if cp.isfinite(r2):\n",
    "                r2s.append(r2)\n",
    "        return float(cp.mean(r2s)) if r2s else -10.0\n",
    "\n",
    "    @staticmethod\n",
    "    def _r2_cp(y_true, y_pred):\n",
    "        y_true = cp.asarray(y_true).reshape(-1)\n",
    "        y_pred = cp.asarray(y_pred).reshape(-1)\n",
    "        ybar = y_true.mean()\n",
    "        ss_res = cp.sum((y_true - y_pred)**2)\n",
    "        ss_tot = cp.sum((y_true - ybar)**2)\n",
    "        return 1.0 - ss_res / (ss_tot + 1e-12)\n",
    "    @staticmethod\n",
    "    def _gpu_poly_features(X_cp: cp.ndarray, degree: int, include_bias: bool = False):\n",
    "        return gpu_poly_features(X_cp, degree, include_bias)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311c47cd-a08d-4746-a6a5-60bb45ab40ca",
   "metadata": {},
   "source": [
    "# Coefficient Recovery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c47e21-94ac-4834-a64e-bb8c78ea170b",
   "metadata": {},
   "source": [
    "The `evaluate_marginal_coefficient_recovery` function is designed to test how accurately a GPU-accelerated APS or MAPS model can recover known coefficients when restricted to main effects. It first builds a simplified version of the chosen model (`VectorizedAPS` or `VectorizedMAPS`) with only single-variable terms and fits it on the provided training data. A helper routine reconstructs polynomial feature names (linear, quadratic, cubic, and binary indicators) so the fitted coefficients can be mapped back to meaningful variable terms. The function then extracts these coefficients, compares them against the supplied ground-truth values, and computes several evaluation metrics: **sign accuracy** (how often the coefficient sign matches), **magnitude correlation** (correlation between true and recovered magnitudes, with a fallback if only one coefficient is available), and **coefficient RMSE** (the root mean squared error across matched terms). All computations are GPU-accelerated with CuPy, and the function is built to gracefully handle missing terms, NaNs, or fitting errors by returning default values with infinite RMSE. This provides a compact yet rigorous way to assess how well APS/MAPS captures marginal effects in practice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1cb04edc-c299-448e-903c-3e45de46b5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_marginal_coefficient_recovery(\n",
    "    ams_model, X_train, y_train, true_coeffs, n_continuous, n_binary\n",
    "):\n",
    "    \"\"\"\n",
    "    GPU version: fits a main-effects-only model using your GPU APS/MAPS\n",
    "    and compares recovered coefficients to the known 'true' ones.\n",
    "    No sklearn; uses the same polynomial naming scheme as the GPU code.\n",
    "    \"\"\"\n",
    "    if ams_model.final_model is None:\n",
    "        return {\n",
    "            'sign_accuracy': 0.0,\n",
    "            'magnitude_correlation': 0.0,\n",
    "            'coefficient_rmse': cp.inf,\n",
    "            'n_compared': 0\n",
    "        }\n",
    "\n",
    "    # helper to mirror GPU poly naming (degree up to 3)\n",
    "    def _poly_feature_names(d, degree):\n",
    "        names = []\n",
    "        # degree 1\n",
    "        for j in range(d):\n",
    "            names.append(f\"x{j}\")\n",
    "        if degree >= 2:\n",
    "            for j in range(d):\n",
    "                names.append(f\"x{j}^2\")\n",
    "            for a in range(d):\n",
    "                for b in range(a+1, d):\n",
    "                    names.append(f\"x{a} x{b}\")\n",
    "        if degree >= 3:\n",
    "            for j in range(d):\n",
    "                names.append(f\"x{j}^3\")\n",
    "            for a in range(d):\n",
    "                for b in range(d):\n",
    "                    if b == a: \n",
    "                        continue\n",
    "                    names.append(f\"x{a}^2 x{b}\")\n",
    "            for a in range(d):\n",
    "                for b in range(a+1, d):\n",
    "                    for c in range(b+1, d):\n",
    "                        names.append(f\"x{a} x{b} x{c}\")\n",
    "        return names\n",
    "\n",
    "    try:\n",
    "        # Build a main-effects-only GPU model of the same family\n",
    "        if ams_model.selected_method == 'APS':\n",
    "            coeff_model = VectorizedAPS(\n",
    "                max_degree=ams_model.max_degree,\n",
    "                max_interaction_order=1,   # main effects only\n",
    "                tolerance=ams_model.tolerance,\n",
    "                verbose=False\n",
    "            )\n",
    "        else:  # MAPS\n",
    "            coeff_model = VectorizedMAPS(\n",
    "                max_degree=ams_model.max_degree,\n",
    "                max_interaction_order=1,   # main effects only\n",
    "                tolerance=ams_model.tolerance,\n",
    "                verbose=False\n",
    "            )\n",
    "\n",
    "        # Fit on GPU (your GPU classes handle CuPy/Torch internally)\n",
    "        coeff_model.fit(X_train, y_train)\n",
    "        inner_model = coeff_model.final_model\n",
    "\n",
    "        # Map selected poly indices -> names for parsing\n",
    "        d_total = n_continuous + n_binary\n",
    "        all_feature_names = _poly_feature_names(d_total, ams_model.max_degree)\n",
    "\n",
    "        fitted_coeffs = {}\n",
    "\n",
    "        # Intercept (TorchLinearRegression exposes .intercept_)\n",
    "        if hasattr(inner_model, 'intercept_'):\n",
    "            fitted_coeffs['intercept'] = float(inner_model.intercept_)\n",
    "\n",
    "        selected_features = list(getattr(inner_model, 'selected_features', []) or [])\n",
    "        model_coeffs = cp.asarray(getattr(inner_model, 'coef_', []), dtype=float)\n",
    "\n",
    "        if selected_features and model_coeffs.size:\n",
    "            for i, feat_idx in enumerate(selected_features):\n",
    "                if i >= model_coeffs.size or feat_idx >= len(all_feature_names):\n",
    "                    continue\n",
    "                feature_name = all_feature_names[feat_idx]\n",
    "                coeff_value = float(model_coeffs[i])\n",
    "\n",
    "                # Only keep main effects (single unique variable in the term)\n",
    "                var_matches = re.findall(r'x(\\d+)', feature_name)\n",
    "                unique_vars = set(var_matches)\n",
    "\n",
    "                if len(unique_vars) <= 1:\n",
    "                    if '^2' in feature_name and ' ' not in feature_name:\n",
    "                        # quadratic of a continuous variable\n",
    "                        var_idx = int(feature_name.split('^')[0][1:])\n",
    "                        if var_idx < n_continuous:\n",
    "                            fitted_coeffs[f'x{var_idx}_quadratic'] = coeff_value\n",
    "                    elif '^3' in feature_name and ' ' not in feature_name:\n",
    "                        # cubic of a continuous variable\n",
    "                        var_idx = int(feature_name.split('^')[0][1:])\n",
    "                        if var_idx < n_continuous:\n",
    "                            fitted_coeffs[f'x{var_idx}_cubic'] = coeff_value\n",
    "                    else:\n",
    "                        # linear/binary main effect\n",
    "                        if feature_name.startswith('x') and '^' not in feature_name and ' ' not in feature_name:\n",
    "                            var_idx = int(feature_name[1:])\n",
    "                            if var_idx < n_continuous:\n",
    "                                fitted_coeffs[f'x{var_idx}_linear'] = coeff_value\n",
    "                            else:\n",
    "                                fitted_coeffs[f'x{var_idx}_binary'] = coeff_value\n",
    "\n",
    "        # Compare to true coefficients\n",
    "        common = [k for k in true_coeffs.keys() if k in fitted_coeffs]\n",
    "        if not common:\n",
    "            return {\n",
    "                'sign_accuracy': 0.0,\n",
    "                'magnitude_correlation': 0.0,\n",
    "                'coefficient_rmse': cp.inf,\n",
    "                'n_compared': 0\n",
    "            }\n",
    "\n",
    "        true_vals = cp.array([true_coeffs[k] for k in common], dtype=float)\n",
    "        fit_vals  = cp.array([fitted_coeffs[k] for k in common], dtype=float)\n",
    "\n",
    "        # drop any NaNs from true values\n",
    "        mask = ~cp.isnan(true_vals)\n",
    "        true_vals = true_vals[mask]\n",
    "        fit_vals  = fit_vals[mask]\n",
    "\n",
    "        if true_vals.size == 0:\n",
    "            return {\n",
    "                'sign_accuracy': 0.0,\n",
    "                'magnitude_correlation': 0.0,\n",
    "                'coefficient_rmse': cp.inf,\n",
    "                'n_compared': 0\n",
    "            }\n",
    "\n",
    "        sign_accuracy = float(cp.mean(cp.sign(true_vals) == cp.sign(fit_vals)))\n",
    "\n",
    "        if true_vals.size > 1:\n",
    "            mag_corr = cp.corrcoef(cp.abs(true_vals), cp.abs(fit_vals))[0, 1]\n",
    "            magnitude_corr = float(0.0 if cp.isnan(mag_corr) else mag_corr)\n",
    "        else:\n",
    "            rel_err = abs(true_vals[0] - fit_vals[0]) / (abs(true_vals[0]) + 1e-10)\n",
    "            magnitude_corr = float(max(0.0, 1.0 - rel_err))\n",
    "\n",
    "        coefficient_rmse = float(cp.sqrt(cp.mean((true_vals - fit_vals) ** 2)))\n",
    "\n",
    "        return {\n",
    "            'sign_accuracy': sign_accuracy,\n",
    "            'magnitude_correlation': magnitude_corr,\n",
    "            'coefficient_rmse': coefficient_rmse,\n",
    "            'n_compared': int(true_vals.size)\n",
    "        }\n",
    "\n",
    "    except Exception:\n",
    "        return {\n",
    "            'sign_accuracy': 0.0,\n",
    "            'magnitude_correlation': 0.0,\n",
    "            'coefficient_rmse': cp.inf,\n",
    "            'n_compared': 0\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3156da5-e26b-4e2b-ac84-61e75e0bf5ca",
   "metadata": {},
   "source": [
    "# Design of Experiments (Half Fraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe79009-7f95-44ee-9466-e3aacd9b97af",
   "metadata": {},
   "source": [
    "This module defines a GPU-accelerated pipeline for **design of experiments (DOE)** and benchmarking interpretable AMS models against black-box baselines. The core generator, `generate_vectorized_half_fraction_design`, creates a set of experimental configurations by combining factorial design matrices (built with `_generate_resolution_iv_design` on the GPU) with different factor levels (continuous, binary, polynomial degree, interaction structures, noise levels, etc.). Invalid setups are filtered using `_is_valid_config`. Each valid configuration is replicated multiple times to form the full DOE. The orchestrator, `run_interpretable_experiments`, then executes the workflow in three stages: (1) **data generation** via `VectorizedBinaryPolynomialFunction` (CuPy arrays produced entirely on GPU), (2) **benchmarking** where interpretable AMS models compete against black-box models in batches, and (3) **analysis** of champion results summarized in a GPU dataframe. Helper routines like `_batch_process_experiments` streamline batch execution, while GPU design construction ensures scalability. Together, these functions allow systematic evaluation of whether AMS can remain interpretable while outperforming traditional black-box approaches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90ec2aa1-8964-49fe-b094-04c517ae096d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_vectorized_half_fraction_design(function_type=None, n_replications=3):\n",
    "    \"\"\"Generate half-fraction design (design matrix on GPU via CuPy).\"\"\"\n",
    "    # Function types\n",
    "    function_types = [function_type] if function_type else ['additive', 'multiplicative', 'mixed']\n",
    "\n",
    "    # Factor levels\n",
    "    factors = {\n",
    "        'n_continuous': [4, 6],\n",
    "        'n_binary': [0, 2],\n",
    "        'degree': [2, 3],\n",
    "        'interaction_order': [1, 2],\n",
    "        'noise_level': [0.2, 0.3],\n",
    "        'binary_effect_strength': [0.5, 1.5],\n",
    "        'n_samples': [500, 1000]\n",
    "    }\n",
    "    interaction_patterns = [\n",
    "        ['continuous'],\n",
    "        ['binary'],\n",
    "        ['continuous', 'binary', 'mixed'],\n",
    "        ['mixed']\n",
    "    ]\n",
    "    factor_names = list(factors.keys())\n",
    "\n",
    "    # GPU design matrix\n",
    "    design_matrix = _generate_resolution_iv_design(len(factor_names))  # cp.ndarray\n",
    "\n",
    "    print(f\"=== VECTORIZED HALF-FRACTION DESIGN ===\")\n",
    "    print(f\"Function types: {len(function_types)}\")\n",
    "    print(f\"Base design matrix: {int(design_matrix.shape[0])} runs x {int(design_matrix.shape[1])} factors\")\n",
    "    print(f\"Interaction patterns: {len(interaction_patterns)}\")\n",
    "    print(f\"Replications per base experiment: {n_replications}\")\n",
    "\n",
    "    base_experiments = []\n",
    "    base_experiment_id = 0\n",
    "\n",
    "    for func_type in function_types:\n",
    "        print(f\"  Processing function type: {func_type}\")\n",
    "        # Iterate rows from GPU; pull scalar bits with .item()\n",
    "        for run_idx in range(int(design_matrix.shape[0])):\n",
    "            factor_levels = design_matrix[run_idx]  # cp array row\n",
    "            for interaction_pattern in interaction_patterns:\n",
    "                base_experiment_id += 1\n",
    "\n",
    "                config = {\n",
    "                    'base_experiment_id': base_experiment_id,\n",
    "                    'function_type': func_type,\n",
    "                    'interaction_types': interaction_pattern\n",
    "                }\n",
    "\n",
    "                for i, factor_name in enumerate(factor_names):\n",
    "                    level_index = int(factor_levels[i].item())\n",
    "                    config[factor_name] = factors[factor_name][level_index]\n",
    "\n",
    "                config['total_features'] = config['n_continuous'] + config['n_binary']\n",
    "\n",
    "                if _is_valid_config(config):\n",
    "                    base_experiments.append(config)\n",
    "\n",
    "    print(f\"  Generated {len(base_experiments)} valid base experiments\")\n",
    "\n",
    "    # Distribution by function type\n",
    "    func_type_counts = {}\n",
    "    for exp in base_experiments:\n",
    "        ft = exp['function_type']\n",
    "        func_type_counts[ft] = func_type_counts.get(ft, 0) + 1\n",
    "    print(f\"  Function type distribution in base: {func_type_counts}\")\n",
    "\n",
    "    # Replicate\n",
    "    all_experiments = []\n",
    "    experiment_id = 0\n",
    "    for base_exp in base_experiments:\n",
    "        for rep in range(n_replications):\n",
    "            experiment_id += 1\n",
    "            replicated_exp = base_exp.copy()\n",
    "            replicated_exp['experiment_id'] = experiment_id\n",
    "            replicated_exp['replication'] = rep\n",
    "            all_experiments.append(replicated_exp)\n",
    "\n",
    "    # Final checks\n",
    "    final_func_type_counts = {}\n",
    "    for exp in all_experiments:\n",
    "        ft = exp['function_type']\n",
    "        final_func_type_counts[ft] = final_func_type_counts.get(ft, 0) + 1\n",
    "\n",
    "    print(f\"=== FINAL DESIGN SUMMARY ===\")\n",
    "    print(f\"Base experiments: {len(base_experiments)}\")\n",
    "    print(f\"Total experiments with replications: {len(all_experiments)}\")\n",
    "    print(f\"Expected total: {len(base_experiments)} × {n_replications} = {len(base_experiments) * n_replications}\")\n",
    "    print(f\"Final function type distribution: {final_func_type_counts}\")\n",
    "\n",
    "    for ft in function_types:\n",
    "        count = final_func_type_counts.get(ft, 0)\n",
    "        expected = func_type_counts.get(ft, 0) * n_replications\n",
    "        print(f\"  {ft}: {count} experiments ({count // n_replications} base × {n_replications} reps)\")\n",
    "        if count != expected:\n",
    "            print(f\"    ⚠️  WARNING: Expected {expected}, got {count}\")\n",
    "\n",
    "    return all_experiments\n",
    "\n",
    "\n",
    "def _generate_resolution_iv_design(n_factors: int) -> cp.ndarray:\n",
    "    \"\"\"\n",
    "    Generate a Resolution IV half-fraction design matrix on GPU (CuPy).\n",
    "    Returns a cp.ndarray of shape (runs, n_factors) with {0,1} entries.\n",
    "    \"\"\"\n",
    "    if n_factors <= 4:\n",
    "        # Full factorial: all 2^k combinations using GPU bit tricks\n",
    "        runs = 1 << n_factors\n",
    "        idx = cp.arange(runs, dtype=cp.uint32)[:, None]                         # (runs, 1)\n",
    "        bitpos = cp.arange(n_factors, dtype=cp.uint32)[None, :]                 # (1, n_factors)\n",
    "        mat = ((idx >> bitpos) & 1).astype(cp.int8)                             # (runs, n_factors)\n",
    "        return mat\n",
    "\n",
    "    # Base 4-factor (A,B,C,D) 2^4 = 16 runs\n",
    "    base_runs = 1 << 4\n",
    "    idx = cp.arange(base_runs, dtype=cp.uint32)[:, None]                        # (16,1)\n",
    "    bitpos = cp.arange(4, dtype=cp.uint32)[None, :]                             # (1,4)\n",
    "    base = ((idx >> bitpos) & 1).astype(cp.int8)                                # (16,4) -> A,B,C,D\n",
    "\n",
    "    # Generators (Resolution IV): E = A⊕B⊕C, F = B⊕C⊕D, G = A⊕C⊕D\n",
    "    A, B, C, D = base[:, 0], base[:, 1], base[:, 2], base[:, 3]\n",
    "    E = (A ^ B ^ C).astype(cp.int8)\n",
    "    F = (B ^ C ^ D).astype(cp.int8)\n",
    "    G = (A ^ C ^ D).astype(cp.int8)\n",
    "\n",
    "    full = cp.concatenate([base, E[:, None], F[:, None], G[:, None]], axis=1)   # (16,7)\n",
    "\n",
    "    # If more than 7 factors requested, pad with zeros; else slice to n_factors\n",
    "    if n_factors <= 7:\n",
    "        return full[:, :n_factors]\n",
    "    else:\n",
    "        zeros = cp.zeros((full.shape[0], n_factors - full.shape[1]), dtype=cp.int8)\n",
    "        return cp.concatenate([full, zeros], axis=1)\n",
    "\n",
    "\n",
    "def _is_valid_config(config) -> bool:\n",
    "    \"\"\"Validate experiment configuration (CPU logic — negligible cost).\"\"\"\n",
    "    n_continuous = config['n_continuous']\n",
    "    n_binary = config['n_binary']\n",
    "    interaction_types = config['interaction_types']\n",
    "    degree = config['degree']\n",
    "\n",
    "    if n_continuous == 0 and n_binary == 0:\n",
    "        return False\n",
    "\n",
    "    if n_binary == 0 and any(itype in ['binary', 'mixed'] for itype in interaction_types):\n",
    "        return False\n",
    "    if n_continuous < 2 and 'continuous' in interaction_types:\n",
    "        return False\n",
    "    if n_binary < 2 and 'binary' in interaction_types:\n",
    "        return False\n",
    "\n",
    "    total_features = n_continuous + n_binary\n",
    "    if total_features > 8 or (total_features > 6 and degree > 2):\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def run_interpretable_experiments(function_type=None):\n",
    "    \"\"\"Top-level orchestration (mostly control flow/IO; heavy math already on GPU elsewhere).\"\"\"\n",
    "    print(\"=== INTERPRETABLE MODEL vs BLACK BOX BENCHMARKS DOE ===\")\n",
    "\n",
    "    experiments = generate_vectorized_half_fraction_design(function_type)\n",
    "\n",
    "    print(f\"\\nExecuting {len(experiments)} experiments...\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # STAGE 1: Data Generation (your VectorizedBinaryPolynomialFunction now emits CuPy on-GPU arrays)\n",
    "    print(\"\\n🔄 STAGE 1: Batch Data Generation\")\n",
    "    batch_data = VectorizedBinaryPolynomialFunction.generate_batch_mixed_data(experiments)\n",
    "\n",
    "    if len(batch_data) == 0:\n",
    "        print(\"❌ No valid data generated\")\n",
    "        return cudf.DataFrame()\n",
    "\n",
    "    print(f\"✅ Generated {len(batch_data)} datasets\")\n",
    "\n",
    "    # STAGE 2: Interpretable Model Evaluation\n",
    "    print(\"\\n🏆 STAGE 2: Interpretable Model vs Black Box Evaluation\")\n",
    "    # Check memory and adapt batch size\n",
    "    free_mem = gpu_memory_check(\"Stage 2 Memory Check\")\n",
    "    \n",
    "    if free_mem > 12000:      # >12GB free\n",
    "        batch_size = min(100, len(batch_data) // 2)\n",
    "    elif free_mem > 8000:     # >8GB free  \n",
    "        batch_size = min(75, len(batch_data) // 3)\n",
    "    elif free_mem > 5000:     # >5GB free\n",
    "        batch_size = min(50, len(batch_data) // 4)\n",
    "    else:                     # <5GB free\n",
    "        batch_size = min(25, len(batch_data) // 6)\n",
    "        print(f\"⚠️ Low memory detected - using small batches of {batch_size}\")\n",
    "    \n",
    "    print(f\"Using batch size: {batch_size} (based on {free_mem:.0f}MB available)\")\n",
    "    results = []\n",
    "\n",
    "    batch_size = min(100, len(batch_data) // 2)  # Larger batches, fewer iterations\n",
    "    n_batches = (len(batch_data) + batch_size - 1) // batch_size\n",
    "\n",
    "    for batch_idx in range(n_batches):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = min(start_idx + batch_size, len(batch_data))\n",
    "        batch = batch_data[start_idx:end_idx]\n",
    "\n",
    "        print(f\"  Processing batch {batch_idx+1}/{n_batches} ({len(batch)} model comparisons)\")\n",
    "\n",
    "        batch_results = _process_single_batch_champion_focused(batch)\n",
    "        results.extend(batch_results)\n",
    "\n",
    "    # STAGE 3: Interpretable ModelAnalysis\n",
    "    print(\"\\n📊 STAGE 3: Interpretable Model Results Analysis\")\n",
    "\n",
    "    if len(results) == 0:\n",
    "        print(\"❌ No interpretable model estimates completed\")\n",
    "        return cudf.DataFrame()\n",
    "\n",
    "    df = cudf.DataFrame(results)\n",
    "    total_time = time.time() - start_time\n",
    "\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"INTERPRETABLE MODEL DOE RESULTS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"🏆 Completed {len(df)} model comparisons\")\n",
    "    print(f\"⏱️  Total runtime: {total_time/60:.1f} minutes\")\n",
    "\n",
    "    analyzed_df = analyze_interpretable_champion_results(df)\n",
    "    return analyzed_df\n",
    "\n",
    "\n",
    "def _batch_process_experiments(batch_data):\n",
    "    results = []\n",
    "    batch_size = 50\n",
    "    n_batches = (len(batch_data) + batch_size - 1) // batch_size\n",
    "\n",
    "    for batch_idx in range(n_batches):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = min(start_idx + batch_size, len(batch_data))\n",
    "        batch = batch_data[start_idx:end_idx]\n",
    "\n",
    "        print(f\"  Processing batch {batch_idx+1}/{n_batches} ({len(batch)} experiments)\")\n",
    "        batch_results = _process_single_batch(batch)\n",
    "        results.extend(batch_results)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e1565c-e06d-4a09-9b5f-f584ea0bfa3c",
   "metadata": {},
   "source": [
    "# Evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3a99a2-06e1-494f-9927-a8b36503ad5e",
   "metadata": {},
   "source": [
    "This module provides a GPU-oriented evaluation framework for comparing an interpretable model (AMS) against GPU baselines (cuML Random Forest and Torch MLP), with all computations performed on the GPU when possible. It is designed to be efficient, neutral in its messaging, and compatible with both pandas and cuDF data structures.\n",
    "\n",
    "The evaluator integrates CuPy as the default GPU backend, falling back to NumPy if no GPU is available. Zero-copy bridges are implemented between CuPy and Torch via `dlpack`, allowing efficient sharing of GPU memory. An on-GPU R² scoring function (`cp_r2_score`) is provided to evaluate model predictions directly on CuPy arrays, with safeguards against division by zero when variance is low. A simple Torch-based multilayer perceptron (MLP) regressor is also included, supporting GPU training with adjustable parameters such as batch size, epochs, learning rate, and weight decay. The regressor includes `fit()` and `predict()` methods that convert data seamlessly between CuPy and Torch.\n",
    "\n",
    "At the core of the system is the `ModelEvaluator` class, which compares the interpretable AMS model with GPU baselines. For AMS, it reports R² scores, overfitting measures, and interpretability metrics such as coefficient sign accuracy, correlation, and RMSE. For the GPU baselines, it evaluates both cuML Random Forest and the Torch MLP, computing benchmark statistics including maximum and mean R² values. The evaluator then provides a neutral comparative analysis, determining whether AMS outperforms, matches, or lags behind the baselines. Interpretability is assessed based on correlation thresholds, and each evaluation is assigned a status level ranging from STRONG and COMPETITIVE to NEAR, INTERPRETABLE_ONLY, or WEAK. For backward compatibility, the `InterpretableChampionEvaluator` alias is maintained.\n",
    "\n",
    "The framework also supports batch evaluations through the `_process_single_batch_eval` function, which performs GPU-based train/test splits, fits AMS models, retrieves true coefficients, and runs evaluations using the `ModelEvaluator`. Metadata such as method scores, feature counts, and interaction strengths are included in the results. A backward-compatible alias `_process_single_batch_champion_focused` is provided. Results are then analyzed through the `analyze_results` function, which summarizes distributions of status levels and selected methods, along with mean scores for AMS R², benchmark R², and interpretability. This analysis works with both pandas and cuDF data and is accessible through a compatibility wrapper named `analyze_interpretable_champion_results`.\n",
    "\n",
    "In summary, this evaluator offers a GPU-first, neutral, and extensible framework for training and comparing interpretable and black-box models. It measures predictive performance, assesses overfitting, evaluates interpretability through coefficient recovery, and delivers neutral status tags that indicate the competitiveness of the AMS model relative to GPU baselines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f163fb9-67ba-4dad-a2d6-05f69c4d0b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === GPU-first evaluator (cuML + PyTorch), neutral messaging ===\n",
    "\n",
    "\n",
    "try:\n",
    "    import cupy as cp\n",
    "    _HAS_CUPY = True\n",
    "except ImportError:\n",
    "    import numpy as cp  # fallback to NumPy if no GPU\n",
    "    _HAS_CUPY = False\n",
    "\n",
    "# --- zero-copy bridges (Torch <-> CuPy) ---\n",
    "import torch.utils.dlpack as dlpack\n",
    "\n",
    "def cupy_to_torch(x_cp: cp.ndarray) -> torch.Tensor:\n",
    "    \"\"\"Convert CuPy array to PyTorch tensor (version-agnostic)\"\"\"\n",
    "    try:\n",
    "        # Try new method name first (CuPy >= 9.0)\n",
    "        if hasattr(x_cp, 'to_dlpack'):\n",
    "            return dlpack.from_dlpack(x_cp.to_dlpack())\n",
    "        # Fall back to old method name (CuPy < 9.0)\n",
    "        elif hasattr(x_cp, 'toDlpack'):\n",
    "            return dlpack.from_dlpack(x_cp.toDlpack())\n",
    "        else:\n",
    "            raise AttributeError(\"CuPy array has no DLPack method\")\n",
    "    except Exception as e:\n",
    "        print(f\"DLPack conversion failed, using CPU fallback: {e}\")\n",
    "        # CPU fallback (slower but reliable)\n",
    "        return torch.from_numpy(cp.asnumpy(x_cp)).cuda()\n",
    "\n",
    "def torch_to_cupy(x_th: torch.Tensor) -> cp.ndarray:\n",
    "    \"\"\"Convert PyTorch tensor to CuPy array (version-agnostic)\"\"\"\n",
    "    try:\n",
    "        # Try new method name first\n",
    "        if hasattr(cp, 'from_dlpack'):\n",
    "            return cp.from_dlpack(dlpack.to_dlpack(x_th))\n",
    "        # Fall back to old method name\n",
    "        elif hasattr(cp, 'fromDlpack'):\n",
    "            return cp.fromDlpack(dlpack.to_dlpack(x_th))\n",
    "        else:\n",
    "            raise AttributeError(\"CuPy has no DLPack method\")\n",
    "    except Exception as e:\n",
    "        print(f\"DLPack conversion failed, using CPU fallback: {e}\")\n",
    "        # CPU fallback\n",
    "        return cp.asarray(x_th.detach().cpu().numpy())\n",
    "\n",
    "# --- on-GPU R^2 (works on 1-D CuPy arrays) ---\n",
    "def cp_r2_score(y_true, y_pred):\n",
    "    # Robust, CuPy-native R^2 that matches the inline formula you debugged\n",
    "    yt = cp.asarray(y_true, dtype=cp.float32).reshape(-1)\n",
    "    yp = cp.asarray(y_pred, dtype=cp.float32).reshape(-1)\n",
    "    if yt.shape != yp.shape:\n",
    "        raise ValueError(f\"cp_r2_score: shape mismatch: {yt.shape} vs {yp.shape}\")\n",
    "    if not cp.isfinite(yt).all() or not cp.isfinite(yp).all():\n",
    "        return float('nan')  # don’t silently produce nonsense\n",
    "    ybar   = cp.mean(yt)\n",
    "    ss_res = cp.sum((yt - yp) ** 2)\n",
    "    ss_tot = cp.sum((yt - ybar) ** 2)\n",
    "    return float(1.0 - ss_res / (ss_tot + 1e-12))\n",
    "\n",
    "# --- a small Torch MLP regressor (GPU) ---\n",
    "class TorchMLPRegressor(nn.Module):\n",
    "    def __init__(self, in_dim: int, hidden=(128, 64, 32), dropout=0.2):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        last = in_dim\n",
    "        \n",
    "        for i, h in enumerate(hidden):\n",
    "            layers += [\n",
    "                nn.Linear(last, h),\n",
    "                nn.BatchNorm1d(h),  # ✅ Helps with training stability\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout) if i < len(hidden)-1 else nn.Identity()\n",
    "            ]\n",
    "            last = h\n",
    "        \n",
    "        layers += [nn.Linear(last, 1)]\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        \n",
    "        # ✅ Better weight initialization\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "\n",
    "    def fit(self, X_cp: cp.ndarray, y_cp: cp.ndarray,\n",
    "            epochs=200, batch_size=1024, lr=1e-3, weight_decay=1e-4,\n",
    "            verbose=False, device=None):\n",
    "        if device is None:\n",
    "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        X_th = cupy_to_torch(X_cp).float().to(device)\n",
    "        y_th = cupy_to_torch(y_cp).float().view(-1, 1).to(device)\n",
    "\n",
    "        ds = TensorDataset(X_th, y_th)\n",
    "        dl = DataLoader(ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        self.to(device)\n",
    "        opt = torch.optim.Adam(self.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        loss_fn = nn.MSELoss()\n",
    "\n",
    "        try:\n",
    "            self.train()\n",
    "            for ep in range(epochs):\n",
    "                ep_loss = 0.0\n",
    "                for xb, yb in dl:\n",
    "                    opt.zero_grad(set_to_none=True)\n",
    "                    pred = self.net(xb)\n",
    "                    loss = loss_fn(pred, yb)\n",
    "                    loss.backward()\n",
    "                    opt.step()\n",
    "                    ep_loss += loss.item()  # ✅ More efficient than .detach().cpu()\n",
    "                \n",
    "                # Optional: periodic cleanup\n",
    "                if ep % 50 == 0:\n",
    "                    torch.cuda.empty_cache()\n",
    "                    \n",
    "        finally:\n",
    "            # ✅ Always cleanup\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        return self\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def predict(self, X_cp: cp.ndarray, device=None) -> cp.ndarray:\n",
    "        if device is None:\n",
    "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.eval().to(device)\n",
    "        X_th = cupy_to_torch(X_cp).float().to(device)\n",
    "        pred = self.net(X_th).squeeze(1)\n",
    "        return torch_to_cupy(pred)\n",
    "\n",
    "# === Evaluator (neutral language) ===\n",
    "class ModelEvaluator:\n",
    "    \"\"\"\n",
    "    Compare the interpretable model (AMS) vs GPU baselines (cuML RF, Torch MLP)\n",
    "    with all computations on GPU where possible.\n",
    "    \"\"\"\n",
    "    def __init__(self, random_state: int = 42):\n",
    "        self.random_state = random_state\n",
    "        self.rf_model = None\n",
    "        self.nn_model = None\n",
    "\n",
    "    def evaluate_models(self, X_train, X_test, y_train, y_test,\n",
    "                        ams_model, true_coeffs, feature_info):\n",
    "        \"\"\"\n",
    "        X_*, y_*: CuPy arrays\n",
    "        ams_model: your fitted VectorizedAPS/VectorizedMAPS wrapper\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "\n",
    "        # 1) Interpretable model (AMS)\n",
    "        ams_results = self._evaluate_ams(X_train, X_test, y_train, y_test,\n",
    "                                         ams_model, true_coeffs, feature_info)\n",
    "        results.update(ams_results)\n",
    "\n",
    "        # 2) GPU baselines\n",
    "        blackbox_results = self._evaluate_gpu_baselines(X_train, X_test, y_train, y_test)\n",
    "        results.update(blackbox_results)\n",
    "\n",
    "        # 3) Analysis\n",
    "        analysis = self._analyze_against_benchmarks(results)\n",
    "        results.update(analysis)\n",
    "\n",
    "        return results\n",
    "\n",
    "    def _evaluate_ams(self, X_train, X_test, y_train, y_test,\n",
    "                      ams_model, true_coeffs, feature_info):\n",
    "            # Time AMS predictions\n",
    "        start_time = time.time()\n",
    "        ams_pred_test = ams_model.predict(X_test)\n",
    "        ams_pred_train = ams_model.predict(X_train)\n",
    "        ams_predict_time = time.time() - start_time\n",
    "        ams_pred_test = ams_model.predict(X_test)\n",
    "        ams_pred_train = ams_model.predict(X_train)\n",
    "\n",
    "        # Ensure CuPy\n",
    "        if not isinstance(ams_pred_test, cp.ndarray):\n",
    "            ams_pred_test = cp.asarray(ams_pred_test)\n",
    "        if not isinstance(ams_pred_train, cp.ndarray):\n",
    "            ams_pred_train = cp.asarray(ams_pred_train)\n",
    "\n",
    "        ams_r2_test = cp_r2_score(y_test, ams_pred_test)\n",
    "        ams_r2_train = cp_r2_score(y_train, ams_pred_train)\n",
    "        ams_overfit = ams_r2_train - ams_r2_test\n",
    "\n",
    "        # Time coefficient recovery\n",
    "        start_time = time.time()\n",
    "        coeff_recovery = evaluate_marginal_coefficient_recovery_fixed(\n",
    "            ams_model, X_train, y_train, true_coeffs,\n",
    "            feature_info['n_continuous'], feature_info['n_binary']\n",
    "        )\n",
    "        coeff_recovery_time = time.time() - start_time\n",
    "\n",
    "        # Interpretability metrics (your GPU-ized function)\n",
    "        coeff_recovery = evaluate_marginal_coefficient_recovery_fixed(\n",
    "            ams_model, X_train, y_train, true_coeffs,\n",
    "            feature_info['n_continuous'], feature_info['n_binary']\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'ams_r2': ams_r2_test,\n",
    "            'ams_r2_train': ams_r2_train,\n",
    "            'ams_overfit': ams_overfit,\n",
    "            'ams_method': ams_model.selected_method,\n",
    "\n",
    "            # interpretability (unchanged schema)\n",
    "            'ams_coeff_sign_accuracy': coeff_recovery['sign_accuracy'],\n",
    "            'ams_coeff_magnitude_correlation': coeff_recovery['magnitude_correlation'],\n",
    "            'ams_coeff_rmse': coeff_recovery['coefficient_rmse'],\n",
    "            'ams_coeffs_recovered': coeff_recovery['n_compared'],\n",
    "\n",
    "            # simple interpretability flags\n",
    "            'ams_strong_interpretability': coeff_recovery['magnitude_correlation'] > 0.9,\n",
    "            'ams_good_interpretability': coeff_recovery['magnitude_correlation'] > 0.7,\n",
    "            'ams_interpretability_score': coeff_recovery['magnitude_correlation'],\n",
    "        }\n",
    "\n",
    "    def _evaluate_gpu_baselines(self, X_train, X_test, y_train, y_test):\n",
    "        # --- cuML Random Forest ---\n",
    "        rf_res = self._eval_rf_cuml(X_train, X_test, y_train, y_test)\n",
    "\n",
    "        # --- Torch MLP (with simple on-GPU standardization) ---\n",
    "        nn_res = self._eval_torch_mlp(X_train, X_test, y_train, y_test)\n",
    "\n",
    "        rf_r2 = float(rf_res.get('rf_r2', float('nan')))\n",
    "        nn_r2 = float(nn_res.get('nn_r2', float('nan')))\n",
    "        \n",
    "        benchmark_r2_max  = max(rf_r2, nn_r2)\n",
    "        benchmark_r2_mean = (rf_r2 + nn_r2) / 2.0\n",
    "\n",
    "        out = {}\n",
    "        out.update(rf_res)\n",
    "        out.update(nn_res)\n",
    "        out.update({\n",
    "            'benchmark_r2_max': benchmark_r2_max,\n",
    "            'benchmark_r2_mean': benchmark_r2_mean,\n",
    "            'benchmark_best_method': 'RF' if rf_res['rf_r2'] > nn_res['nn_r2'] else 'NN'\n",
    "        })\n",
    "        return out\n",
    "\n",
    "    def _eval_rf_cuml(self, X_train, X_test, y_train, y_test):\n",
    "        try:\n",
    "            self.rf_model = cuRF(\n",
    "                n_estimators=100,\n",
    "                max_depth=10,\n",
    "                min_samples_split=5,\n",
    "                min_samples_leaf=2,\n",
    "                random_state=self.random_state,\n",
    "                n_streams=8,\n",
    "            )\n",
    "            self.rf_model.fit(X_train, y_train)\n",
    "    \n",
    "            # Predictions\n",
    "            rf_pred_test  = self.rf_model.predict(X_test)\n",
    "            rf_pred_train = self.rf_model.predict(X_train)\n",
    "    \n",
    "            # Ensure CuPy arrays for scoring\n",
    "            rf_pred_test  = cp.asarray(rf_pred_test)\n",
    "            rf_pred_train = cp.asarray(rf_pred_train)\n",
    "    \n",
    "            # R² using your cp_r2_score (same formula as NN)\n",
    "            rf_r2        = cp_r2_score(y_test,  rf_pred_test)\n",
    "            rf_r2_train  = cp_r2_score(y_train, rf_pred_train)\n",
    "            rf_overfit   = float(rf_r2_train - rf_r2)\n",
    "    \n",
    "            return {\n",
    "                'rf_r2': rf_r2,\n",
    "                'rf_r2_train': rf_r2_train,\n",
    "                'rf_overfit': rf_overfit,\n",
    "                'rf_success': True,\n",
    "            }\n",
    "    \n",
    "        except Exception as e:\n",
    "            print(f\"🚨 RF Eval: Exception caught: {type(e).__name__}: {e}\")\n",
    "            import traceback; traceback.print_exc()\n",
    "            return {\n",
    "                'rf_r2': float('nan'),\n",
    "                'rf_r2_train': float('nan'),\n",
    "                'rf_overfit': float('nan'),\n",
    "                'rf_success': False,\n",
    "            }\n",
    "    \n",
    "    \n",
    "\n",
    "    def _eval_torch_mlp(self, X_train, X_test, y_train, y_test):\n",
    "        try:\n",
    "            # Ensure proper data types\n",
    "            X_train = cp.ascontiguousarray(X_train.astype(cp.float32))\n",
    "            X_test = cp.ascontiguousarray(X_test.astype(cp.float32))\n",
    "            y_train = cp.ascontiguousarray(y_train.astype(cp.float32))\n",
    "            y_test = cp.ascontiguousarray(y_test.astype(cp.float32))\n",
    "    \n",
    "            # Validate data\n",
    "            if not cp.all(cp.isfinite(X_train)) or not cp.all(cp.isfinite(y_train)):\n",
    "                return {\n",
    "                    'nn_r2': -10.0,\n",
    "                    'nn_r2_train': -10.0,\n",
    "                    'nn_overfit': 0.0,\n",
    "                    'nn_success': False\n",
    "                }\n",
    "    \n",
    "            if X_train.shape[0] < 10:\n",
    "                return {\n",
    "                    'nn_r2': -10.0,\n",
    "                    'nn_r2_train': -10.0,\n",
    "                    'nn_overfit': 0.0,\n",
    "                    'nn_success': False\n",
    "                }\n",
    "    \n",
    "            # Force positivity (like MAPS does)\n",
    "            y_min = float(cp.min(y_train))\n",
    "            if y_min <= 0.0:\n",
    "                shift = abs(y_min) + 1.0\n",
    "                y_train_pos = y_train + shift\n",
    "                y_test_pos = y_test + shift\n",
    "            else:\n",
    "                shift = 0.0\n",
    "                y_train_pos = y_train\n",
    "                y_test_pos = y_test\n",
    "    \n",
    "            # Train in log-space for multiplicative stability\n",
    "            y_train_log = cp.log(y_train_pos)\n",
    "    \n",
    "            # Standardize features\n",
    "            mu = cp.mean(X_train, axis=0)\n",
    "            sigma = cp.std(X_train, axis=0)\n",
    "            sigma = cp.where(sigma < 1e-6, 1.0, sigma)\n",
    "            \n",
    "            Xtr = (X_train - mu) / sigma\n",
    "            Xte = (X_test - mu) / sigma\n",
    "    \n",
    "            # Create and train model\n",
    "            in_dim = Xtr.shape[1]\n",
    "            hidden_size = min(64, max(32, in_dim * 4))\n",
    "            self.nn_model = TorchMLPRegressor(in_dim, hidden=(hidden_size, hidden_size//2))\n",
    "            \n",
    "            # Train on log-transformed targets\n",
    "            self.nn_model.fit(\n",
    "                Xtr, y_train_log, \n",
    "                epochs=250, \n",
    "                batch_size=min(512, max(64, X_train.shape[0]//4)), \n",
    "                lr=5e-4, \n",
    "                weight_decay=1e-5\n",
    "            )\n",
    "    \n",
    "            # Predict in log-space, then back-transform\n",
    "            log_pred_test = self.nn_model.predict(Xte)\n",
    "            log_pred_train = self.nn_model.predict(Xtr)\n",
    "            \n",
    "            # Back-transform to original scale\n",
    "            nn_pred_test = cp.exp(log_pred_test) - shift\n",
    "            nn_pred_train = cp.exp(log_pred_train) - shift\n",
    "    \n",
    "            # Validate predictions\n",
    "            if not cp.all(cp.isfinite(nn_pred_test)) or not cp.all(cp.isfinite(nn_pred_train)):\n",
    "                return {\n",
    "                    'nn_r2': -10.0,\n",
    "                    'nn_r2_train': -10.0,\n",
    "                    'nn_overfit': 0.0,\n",
    "                    'nn_success': False\n",
    "                }\n",
    "    \n",
    "            # Calculate R² on original scale\n",
    "            nn_r2 = cp_r2_score(y_test, nn_pred_test)\n",
    "            nn_r2_train = cp_r2_score(y_train, nn_pred_train)\n",
    "            \n",
    "            # Validate R² values\n",
    "            if not cp.isfinite(nn_r2):\n",
    "                nn_r2 = -10.0\n",
    "            if not cp.isfinite(nn_r2_train):\n",
    "                nn_r2_train = -10.0\n",
    "    \n",
    "            nn_overfit = float(nn_r2_train - nn_r2)\n",
    "    \n",
    "            return {\n",
    "                'nn_r2': float(nn_r2),\n",
    "                'nn_r2_train': float(nn_r2_train),\n",
    "                'nn_overfit': nn_overfit,\n",
    "                'nn_success': True\n",
    "            }\n",
    "    \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'nn_r2': -10.0,\n",
    "                'nn_r2_train': -10.0,\n",
    "                'nn_overfit': 0.0,\n",
    "                'nn_success': False\n",
    "            }\n",
    "\n",
    "\n",
    "    def _analyze_against_benchmarks(self, results: dict):\n",
    "        ams_r2 = results['ams_r2']\n",
    "        rf_r2 = results['rf_r2']\n",
    "        nn_r2 = results['nn_r2']\n",
    "        bmax = results['benchmark_r2_max']\n",
    "        bmean = results['benchmark_r2_mean']\n",
    "        interp = results['ams_interpretability_score']\n",
    "\n",
    "        beats_rf = ams_r2 > rf_r2\n",
    "        beats_nn = ams_r2 > nn_r2\n",
    "        beats_best = ams_r2 > bmax\n",
    "        beats_avg  = ams_r2 > bmean\n",
    "\n",
    "        gap_vs_rf = ams_r2 - rf_r2\n",
    "        gap_vs_nn = ams_r2 - nn_r2\n",
    "        gap_vs_best = ams_r2 - bmax\n",
    "        gap_vs_avg  = ams_r2 - bmean\n",
    "\n",
    "        # neutral status tags\n",
    "        interpretable_and_outperforms = (interp > 0.8) and (ams_r2 > bmax)\n",
    "        interpretable_and_competitive = (interp > 0.8) and (gap_vs_best > -0.05)  # within 5%\n",
    "\n",
    "        # simple status level\n",
    "        if interp > 0.8 and gap_vs_best > 0.02:\n",
    "            status_level = 'STRONG'\n",
    "        elif interp > 0.8 and gap_vs_best > -0.02:\n",
    "            status_level = 'COMPETITIVE'\n",
    "        elif interp > 0.8 and gap_vs_best > -0.05:\n",
    "            status_level = 'NEAR'\n",
    "        elif interp > 0.8:\n",
    "            status_level = 'INTERPRETABLE_ONLY'\n",
    "        else:\n",
    "            status_level = 'WEAK'\n",
    "\n",
    "        # keep a neutral boolean; also include legacy key for compatibility\n",
    "        success = status_level in ['STRONG', 'COMPETITIVE']\n",
    "\n",
    "        return {\n",
    "            'beats_rf': beats_rf,\n",
    "            'beats_nn': beats_nn,\n",
    "            'beats_best': beats_best,\n",
    "            'beats_avg': beats_avg,\n",
    "\n",
    "            'gap_vs_rf': gap_vs_rf,\n",
    "            'gap_vs_nn': gap_vs_nn,\n",
    "            'gap_vs_best': gap_vs_best,\n",
    "            'gap_vs_avg': gap_vs_avg,\n",
    "\n",
    "            'interpretable_and_outperforms': interpretable_and_outperforms,\n",
    "            'interpretable_and_competitive': interpretable_and_competitive,\n",
    "\n",
    "            'status_level': status_level,\n",
    "            'success': success,\n",
    "\n",
    "            # legacy compatibility fields (you can remove once you update downstream code)\n",
    "            'ams_beats_rf': beats_rf,\n",
    "            'ams_beats_nn': beats_nn,\n",
    "            'ams_beats_best_blackbox': beats_best,\n",
    "            'ams_beats_avg_blackbox': beats_avg,\n",
    "            'ams_vs_rf_gap': gap_vs_rf,\n",
    "            'ams_vs_nn_gap': gap_vs_nn,\n",
    "            'ams_vs_best_blackbox_gap': gap_vs_best,\n",
    "            'ams_vs_avg_blackbox_gap': gap_vs_avg,\n",
    "            'champion_level': status_level,\n",
    "            'champion_success': success,\n",
    "        }\n",
    "\n",
    "# Backward-compat class alias \n",
    "InterpretableChampionEvaluator = ModelEvaluator\n",
    "\n",
    "# === Batch processing (GPU split + neutral logs); keep old name as alias ===\n",
    "\n",
    "def _process_single_batch_eval(batch):\n",
    "    \"\"\"\n",
    "    GPU-oriented batch loop with systematic memory cleanup.\n",
    "    \"\"\"\n",
    "    batch_results = []\n",
    "    \n",
    "    # Initial cleanup\n",
    "    gpu_cleanup(verbose=True)\n",
    "    \n",
    "    for i, (X, y_true, y_noisy, feature_info, config) in enumerate(batch):\n",
    "        try:\n",
    "            # Periodic cleanup every 10 experiments\n",
    "            if i % 10 == 0 and i > 0:\n",
    "                gpu_cleanup(verbose=True)\n",
    "                gpu_memory_check(f\"Batch progress {i}/{len(batch)}\")\n",
    "            \n",
    "            n = X.shape[0]\n",
    "            if n < 20:\n",
    "                continue\n",
    "\n",
    "            # GPU train/test split\n",
    "            rs = cp.random.RandomState(42 + int(config['experiment_id']))\n",
    "            idx = rs.permutation(n)\n",
    "            split = int(n * 0.8)\n",
    "            tr, te = idx[:split], idx[split:]\n",
    "\n",
    "            X_train, X_test = X[tr], X[te]\n",
    "            y_train, y_test = y_noisy[tr], y_noisy[te]\n",
    "\n",
    "            # Fit AMS\n",
    "            ams_model = VectorizedAdaptiveMethodSelector(\n",
    "                max_degree=config['degree'],\n",
    "                max_interaction_order=config['interaction_order'],\n",
    "                verbose=False\n",
    "            )\n",
    "            ams_model.fit(X_train, y_train)\n",
    "\n",
    "            # True coefficients for interpretability eval\n",
    "            true_coeffs = VectorizedBinaryPolynomialFunction.get_true_marginal_coefficients_fixed(\n",
    "                config['n_continuous'], config['n_binary'],\n",
    "                config['binary_effect_strength'], config['function_type'],\n",
    "                X_sample=X_train, y_sample=y_train\n",
    "            )\n",
    "\n",
    "            evaluator = ModelEvaluator(random_state=42 + int(config['experiment_id']))\n",
    "            eval_results = evaluator.evaluate_models(\n",
    "                X_train, X_test, y_train, y_test, ams_model, true_coeffs, feature_info\n",
    "            )\n",
    "            \n",
    "            # ENHANCED REAL-TIME LOGGING WITH METHOD SELECTION\n",
    "            if i % 10 == 0:\n",
    "                ams_r2 = eval_results.get('ams_r2', 0)\n",
    "                rf_r2 = eval_results.get('rf_r2', 0) \n",
    "                nn_r2 = eval_results.get('nn_r2', 0)\n",
    "                status = eval_results.get('status_level', 'UNKNOWN')\n",
    "                interp = eval_results.get('ams_interpretability_score', 0)\n",
    "                method = eval_results.get('ams_method', 'UNKNOWN')  # ADD THIS\n",
    "                \n",
    "                print(f\"    Exp {config['experiment_id']:3d} ({config['function_type']:4s}→{method:4s}): \"\n",
    "                      f\"AMS={ams_r2:.3f} RF={rf_r2:.3f} NN={nn_r2:.3f} \"\n",
    "                      f\"Status={status} Interp={interp:.3f}\")\n",
    "\n",
    "            # expected method\n",
    "            expected_method = (\n",
    "                'APS' if config['function_type'] == 'additive'\n",
    "                else 'MAPS' if config['function_type'] == 'multiplicative'\n",
    "                else 'EITHER'\n",
    "            )\n",
    "            ams_correct = (ams_model.selected_method == expected_method) or (expected_method == 'EITHER')\n",
    "\n",
    "            result = dict(config)\n",
    "            result.update(eval_results)\n",
    "            result.update({\n",
    "                'expected_method': expected_method,\n",
    "                'ams_correct': ams_correct,\n",
    "                'multiplicative_evidence': ams_model.method_scores['multiplicative_evidence'],\n",
    "                'interaction_strength': ams_model.method_scores['interaction_strength'],\n",
    "                'separability': ams_model.method_scores['multiplicative_separability'],\n",
    "                'dynamic_range': ams_model.method_scores['dynamic_range'],\n",
    "                'total_features': config['n_continuous'] + config['n_binary'],\n",
    "                'has_binary': config['n_binary'] > 0,\n",
    "                'is_mixed_vars': (config['n_continuous'] > 0) and (config['n_binary'] > 0),\n",
    "                'n_train': int(split),\n",
    "                'n_test': int(n - split),\n",
    "            })\n",
    "            batch_results.append(result)\n",
    "\n",
    "            # Extra cleanup for memory-intensive mixed functions\n",
    "            if config['function_type'] == 'mixed':\n",
    "                del X_train, X_test, y_train, y_test, ams_model, eval_results\n",
    "                gpu_cleanup()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"    Error in experiment {config['experiment_id']}: {str(e)[:120]}...\")\n",
    "            # Force cleanup on error\n",
    "            gpu_cleanup(verbose=True)\n",
    "            continue\n",
    "\n",
    "    # Final cleanup\n",
    "    gpu_cleanup(verbose=True)\n",
    "    return batch_results\n",
    "\n",
    "# Backward-compat alias\n",
    "def _process_single_batch_champion_focused(batch):\n",
    "    return _process_single_batch_eval(batch)\n",
    "\n",
    "\n",
    "# === Neutral result analysis (with compat wrapper) ===\n",
    "\n",
    "# --- tiny helper so code works with both cuDF and pandas ---\n",
    "def _iter_kv(s):\n",
    "    \"\"\"Yield (key, value) pairs from a pandas or cuDF Series without\n",
    "    forcing a full DataFrame → pandas conversion.\"\"\"\n",
    "    # cuDF path\n",
    "    if hasattr(s, \"values_host\"):  # cuDF Series\n",
    "        keys = s.index.to_pandas().tolist()\n",
    "        vals = s.values_host.tolist()\n",
    "        return zip(keys, vals)\n",
    "    # pandas path\n",
    "    return s.items()\n",
    "\n",
    "def analyze_results(df):\n",
    "    \"\"\"\n",
    "    GPU-safe printing/summary. Works whether df is cuDF or pandas.\n",
    "    \"\"\"\n",
    "    n = len(df)\n",
    "    print(\"\\n================ RESULTS SUMMARY ================\")\n",
    "    print(f\"Rows: {n}\")\n",
    "\n",
    "    # Example 1: status_level distribution (this was failing)\n",
    "    level_counts = df[\"status_level\"].value_counts().sort_index()\n",
    "    print(\"\\nStatus level counts:\")\n",
    "    for level, count in _iter_kv(level_counts):\n",
    "        pct = 100 * count / max(n, 1)\n",
    "        print(f\"  {str(level):16s}: {int(count)} ({pct:.1f}%)\")\n",
    "\n",
    "    # Example 2: method distribution\n",
    "    if \"ams_method\" in df.columns:\n",
    "        meth_counts = df[\"ams_method\"].value_counts().sort_index()\n",
    "        print(\"\\nMethod selection counts:\")\n",
    "        for meth, count in _iter_kv(meth_counts):\n",
    "            pct = 100 * count / max(n, 1)\n",
    "            print(f\"  {str(meth):16s}: {int(count)} ({pct:.1f}%)\")\n",
    "\n",
    "    # Example 3: simple means (cuDF/pandas both fine)\n",
    "    for col in [\"ams_r2\", \"benchmark_r2_max\", \"ams_interpretability_score\"]:\n",
    "        if col in df.columns:\n",
    "            mean_val = float(df[col].mean())  # cuDF returns scalar-like; cast for safety\n",
    "            print(f\"Mean {col}: {mean_val:.4f}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Backward-compat wrapper (so your existing call names still work)\n",
    "def analyze_interpretable_champion_results(df):\n",
    "    return analyze_results(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0512b4b0-3213-45a5-992f-b203bb0deed8",
   "metadata": {},
   "source": [
    "# Additive Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "978f4ca7-22ab-4d7c-aff3-b96ccca782d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RUN 1: ADDITIVE FUNCTIONS ===\n",
      "GPU Memory Initial: 1437.6MB used, 14865.0MB free (16302.6MB total)\n",
      "=== INTERPRETABLE MODEL vs BLACK BOX BENCHMARKS DOE ===\n",
      "=== VECTORIZED HALF-FRACTION DESIGN ===\n",
      "Function types: 1\n",
      "Base design matrix: 16 runs x 7 factors\n",
      "Interaction patterns: 4\n",
      "Replications per base experiment: 3\n",
      "  Processing function type: additive\n",
      "  Generated 32 valid base experiments\n",
      "  Function type distribution in base: {'additive': 32}\n",
      "=== FINAL DESIGN SUMMARY ===\n",
      "Base experiments: 32\n",
      "Total experiments with replications: 96\n",
      "Expected total: 32 × 3 = 96\n",
      "Final function type distribution: {'additive': 96}\n",
      "  additive: 96 experiments (32 base × 3 reps)\n",
      "\n",
      "Executing 96 experiments...\n",
      "\n",
      "🔄 STAGE 1: Batch Data Generation\n",
      "🔄 Generating data for 96 experiments on GPU (CuPy)…\n",
      "  Processing group 1/16\n",
      "  Processing group 6/16\n",
      "  Processing group 11/16\n",
      "  Processing group 16/16\n",
      "✅ Generated data for 96 valid experiments (GPU)\n",
      "✅ Generated 96 datasets\n",
      "\n",
      "🏆 STAGE 2: Interpretable Model vs Black Box Evaluation\n",
      "GPU Memory Stage 2 Memory Check: 1441.6MB used, 14861.0MB free (16302.6MB total)\n",
      "Using batch size: 48 (based on 14861MB available)\n",
      "  Processing batch 1/2 (48 model comparisons)\n",
      "GPU cleanup completed\n",
      "    Exp   1 (additive→APS ): AMS=0.878 RF=0.804 NN=0.537 Status=STRONG Interp=0.997\n",
      "GPU cleanup completed\n",
      "GPU Memory Batch progress 10/48: 3323.6MB used, 12979.0MB free (16302.6MB total)\n",
      "    Exp  53 (additive→APS ): AMS=0.905 RF=0.668 NN=0.158 Status=STRONG Interp=0.994\n",
      "GPU cleanup completed\n",
      "GPU Memory Batch progress 20/48: 3323.6MB used, 12979.0MB free (16302.6MB total)\n",
      "    Exp  12 (additive→APS ): AMS=0.874 RF=0.833 NN=0.546 Status=STRONG Interp=0.971\n",
      "GPU cleanup completed\n",
      "GPU Memory Batch progress 30/48: 3323.6MB used, 12979.0MB free (16302.6MB total)\n",
      "    Exp  16 (additive→APS ): AMS=0.872 RF=0.803 NN=0.523 Status=STRONG Interp=0.973\n",
      "GPU cleanup completed\n",
      "GPU Memory Batch progress 40/48: 3323.6MB used, 12979.0MB free (16302.6MB total)\n",
      "    Exp  68 (additive→APS ): AMS=0.914 RF=0.630 NN=-0.302 Status=STRONG Interp=0.994\n",
      "GPU cleanup completed\n",
      "  Processing batch 2/2 (48 model comparisons)\n",
      "GPU cleanup completed\n",
      "    Exp  25 (additive→MAPS): AMS=0.907 RF=0.909 NN=0.655 Status=WEAK Interp=0.754\n",
      "GPU cleanup completed\n",
      "GPU Memory Batch progress 10/48: 3323.6MB used, 12979.0MB free (16302.6MB total)\n",
      "    Exp  77 (additive→APS ): AMS=0.951 RF=0.744 NN=-0.436 Status=STRONG Interp=0.992\n",
      "GPU cleanup completed\n",
      "GPU Memory Batch progress 20/48: 3323.6MB used, 12979.0MB free (16302.6MB total)\n",
      "    Exp  36 (additive→APS ): AMS=0.884 RF=0.691 NN=-0.055 Status=STRONG Interp=0.989\n",
      "GPU cleanup completed\n",
      "GPU Memory Batch progress 30/48: 3323.6MB used, 12979.0MB free (16302.6MB total)\n",
      "    Exp  40 (additive→APS ): AMS=0.944 RF=0.846 NN=0.087 Status=STRONG Interp=0.985\n",
      "GPU cleanup completed\n",
      "GPU Memory Batch progress 40/48: 3323.6MB used, 12979.0MB free (16302.6MB total)\n",
      "    Exp  92 (additive→MAPS): AMS=0.916 RF=0.817 NN=0.667 Status=WEAK Interp=0.762\n",
      "GPU cleanup completed\n",
      "\n",
      "📊 STAGE 3: Interpretable Model Results Analysis\n",
      "\n",
      "================================================================================\n",
      "INTERPRETABLE MODEL DOE RESULTS\n",
      "================================================================================\n",
      "🏆 Completed 96 model comparisons\n",
      "⏱️  Total runtime: 8.5 minutes\n",
      "\n",
      "================ RESULTS SUMMARY ================\n",
      "Rows: 96\n",
      "\n",
      "Status level counts:\n",
      "  COMPETITIVE     : 5 (5.2%)\n",
      "  STRONG          : 79 (82.3%)\n",
      "  WEAK            : 12 (12.5%)\n",
      "\n",
      "Method selection counts:\n",
      "  APS             : 82 (85.4%)\n",
      "  MAPS            : 14 (14.6%)\n",
      "Mean ams_r2: 0.9172\n",
      "Mean benchmark_r2_max: 0.8297\n",
      "Mean ams_interpretability_score: 0.9598\n",
      "✅ Additive results saved: 96 experiments\n",
      "GPU cleanup completed\n",
      "GPU Memory After additive cleanup: 3327.6MB used, 12975.0MB free (16302.6MB total)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# RUN 1: ADDITIVE FUNCTIONS ONLY\n",
    "# =============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    import os\n",
    "    import time\n",
    "    \n",
    "    print(\"=== RUN 1: ADDITIVE FUNCTIONS ===\")\n",
    "    gpu_memory_check(\"Initial\")\n",
    "    \n",
    "    # Run additive only\n",
    "    df_additive = run_interpretable_experiments(function_type=\"additive\")\n",
    "    \n",
    "    # Save immediately - FIXED VERSION\n",
    "    df_additive_pandas = df_additive.to_pandas()  # Convert to pandas first\n",
    "    df_additive_pandas.to_csv(\"/mnt/c/users/lfult/OneDrive - bc.edu/desktop/aps/results_additive.csv\", index=False)  # ✅ Use pandas AND add filename\n",
    "    print(f\"✅ Additive results saved: {len(df_additive)} experiments\")\n",
    "    \n",
    "    # Clean up\n",
    "    del df_additive, df_additive_pandas  # Delete both versions\n",
    "    gpu_cleanup(verbose=True)\n",
    "    cp.get_default_memory_pool().free_all_blocks()\n",
    "    gpu_memory_check(\"After additive cleanup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811e704b-9fb2-4413-a3b7-ccda72b82961",
   "metadata": {},
   "source": [
    "# Multiplicative Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d73dfee-efaf-4788-97c4-7c4f2600a882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RUN 2: MULTIPLICATIVE FUNCTIONS ===\n",
      "GPU Memory Initial: 3327.6MB used, 12975.0MB free (16302.6MB total)\n",
      "=== INTERPRETABLE MODEL vs BLACK BOX BENCHMARKS DOE ===\n",
      "=== VECTORIZED HALF-FRACTION DESIGN ===\n",
      "Function types: 1\n",
      "Base design matrix: 16 runs x 7 factors\n",
      "Interaction patterns: 4\n",
      "Replications per base experiment: 3\n",
      "  Processing function type: multiplicative\n",
      "  Generated 32 valid base experiments\n",
      "  Function type distribution in base: {'multiplicative': 32}\n",
      "=== FINAL DESIGN SUMMARY ===\n",
      "Base experiments: 32\n",
      "Total experiments with replications: 96\n",
      "Expected total: 32 × 3 = 96\n",
      "Final function type distribution: {'multiplicative': 96}\n",
      "  multiplicative: 96 experiments (32 base × 3 reps)\n",
      "\n",
      "Executing 96 experiments...\n",
      "\n",
      "🔄 STAGE 1: Batch Data Generation\n",
      "🔄 Generating data for 96 experiments on GPU (CuPy)…\n",
      "  Processing group 1/16\n",
      "  Processing group 6/16\n",
      "  Processing group 11/16\n",
      "  Processing group 16/16\n",
      "✅ Generated data for 96 valid experiments (GPU)\n",
      "✅ Generated 96 datasets\n",
      "\n",
      "🏆 STAGE 2: Interpretable Model vs Black Box Evaluation\n",
      "GPU Memory Stage 2 Memory Check: 3311.6MB used, 12991.0MB free (16302.6MB total)\n",
      "Using batch size: 48 (based on 12991MB available)\n",
      "  Processing batch 1/2 (48 model comparisons)\n",
      "GPU cleanup completed\n",
      "    Exp   1 (multiplicative→MAPS): AMS=0.910 RF=0.813 NN=0.548 Status=STRONG Interp=0.979\n",
      "GPU cleanup completed\n",
      "GPU Memory Batch progress 10/48: 3327.6MB used, 12975.0MB free (16302.6MB total)\n",
      "    Exp  53 (multiplicative→MAPS): AMS=0.886 RF=0.655 NN=-0.355 Status=STRONG Interp=0.968\n",
      "GPU cleanup completed\n",
      "GPU Memory Batch progress 20/48: 3327.6MB used, 12975.0MB free (16302.6MB total)\n",
      "    Exp  12 (multiplicative→MAPS): AMS=0.749 RF=0.748 NN=-0.352 Status=COMPETITIVE Interp=0.970\n",
      "GPU cleanup completed\n",
      "GPU Memory Batch progress 30/48: 3327.6MB used, 12975.0MB free (16302.6MB total)\n",
      "    Exp  16 (multiplicative→MAPS): AMS=0.734 RF=0.612 NN=-1.734 Status=STRONG Interp=0.997\n",
      "GPU cleanup completed\n",
      "GPU Memory Batch progress 40/48: 3329.6MB used, 12973.0MB free (16302.6MB total)\n",
      "    Exp  68 (multiplicative→MAPS): AMS=0.922 RF=0.637 NN=-0.340 Status=STRONG Interp=0.992\n",
      "GPU cleanup completed\n",
      "  Processing batch 2/2 (48 model comparisons)\n",
      "GPU cleanup completed\n",
      "    Exp  25 (multiplicative→MAPS): AMS=0.619 RF=0.792 NN=-1.191 Status=INTERPRETABLE_ONLY Interp=0.993\n",
      "GPU cleanup completed\n",
      "GPU Memory Batch progress 10/48: 3329.6MB used, 12973.0MB free (16302.6MB total)\n",
      "    Exp  77 (multiplicative→MAPS): AMS=0.884 RF=0.555 NN=-1.313 Status=STRONG Interp=0.964\n",
      "GPU cleanup completed\n",
      "GPU Memory Batch progress 20/48: 3329.6MB used, 12973.0MB free (16302.6MB total)\n",
      "    Exp  36 (multiplicative→MAPS): AMS=0.892 RF=0.627 NN=-0.002 Status=STRONG Interp=0.824\n",
      "GPU cleanup completed\n",
      "GPU Memory Batch progress 30/48: 3329.6MB used, 12973.0MB free (16302.6MB total)\n",
      "    Exp  40 (multiplicative→MAPS): AMS=0.936 RF=0.843 NN=0.679 Status=STRONG Interp=0.835\n",
      "GPU cleanup completed\n",
      "GPU Memory Batch progress 40/48: 3329.6MB used, 12973.0MB free (16302.6MB total)\n",
      "    Exp  92 (multiplicative→MAPS): AMS=0.758 RF=0.470 NN=-2.938 Status=STRONG Interp=0.989\n",
      "GPU cleanup completed\n",
      "\n",
      "📊 STAGE 3: Interpretable Model Results Analysis\n",
      "\n",
      "================================================================================\n",
      "INTERPRETABLE MODEL DOE RESULTS\n",
      "================================================================================\n",
      "🏆 Completed 96 model comparisons\n",
      "⏱️  Total runtime: 6.2 minutes\n",
      "\n",
      "================ RESULTS SUMMARY ================\n",
      "Rows: 96\n",
      "\n",
      "Status level counts:\n",
      "  COMPETITIVE     : 7 (7.3%)\n",
      "  INTERPRETABLE_ONLY: 5 (5.2%)\n",
      "  NEAR            : 3 (3.1%)\n",
      "  STRONG          : 79 (82.3%)\n",
      "  WEAK            : 2 (2.1%)\n",
      "\n",
      "Method selection counts:\n",
      "  MAPS            : 96 (100.0%)\n",
      "Mean ams_r2: 0.8442\n",
      "Mean benchmark_r2_max: 0.7049\n",
      "Mean ams_interpretability_score: 0.9654\n",
      "✅ Multiplicative results saved: 96 experiments\n",
      "GPU cleanup completed\n",
      "GPU Memory After multiplicative cleanup: 3329.6MB used, 12973.0MB free (16302.6MB total)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import os\n",
    "    import time\n",
    "    \n",
    "    print(\"=== RUN 2: MULTIPLICATIVE FUNCTIONS ===\")\n",
    "    gpu_memory_check(\"Initial\")\n",
    "    \n",
    "    # Run multiplicative only\n",
    "    df_multiplicative = run_interpretable_experiments(function_type=\"multiplicative\")\n",
    "    \n",
    "    # Save immediately\n",
    "    df_multiplicative_pandas = df_multiplicative.to_pandas() \n",
    "    df_multiplicative_pandas.to_csv(\"/mnt/c/users/lfult/OneDrive - bc.edu/desktop/aps/results_multiplicative.csv\", index=False)\n",
    "    print(f\"✅ Multiplicative results saved: {len(df_multiplicative)} experiments\")\n",
    "    \n",
    "    # Clean up\n",
    "    del df_multiplicative\n",
    "    gpu_cleanup(verbose=True)\n",
    "    cp.get_default_memory_pool().free_all_blocks()\n",
    "    gpu_memory_check(\"After multiplicative cleanup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3545652e-2630-46de-8376-8c4e6315df43",
   "metadata": {},
   "source": [
    "# Mixed Function Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "94878966-a8b0-4255-a5b7-37c9244eb89f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RUN 3: MIXED FUNCTIONS ===\n",
      "GPU Memory Initial: 3329.6MB used, 12973.0MB free (16302.6MB total)\n",
      "=== INTERPRETABLE MODEL vs BLACK BOX BENCHMARKS DOE ===\n",
      "=== VECTORIZED HALF-FRACTION DESIGN ===\n",
      "Function types: 1\n",
      "Base design matrix: 16 runs x 7 factors\n",
      "Interaction patterns: 4\n",
      "Replications per base experiment: 3\n",
      "  Processing function type: mixed\n",
      "  Generated 32 valid base experiments\n",
      "  Function type distribution in base: {'mixed': 32}\n",
      "=== FINAL DESIGN SUMMARY ===\n",
      "Base experiments: 32\n",
      "Total experiments with replications: 96\n",
      "Expected total: 32 × 3 = 96\n",
      "Final function type distribution: {'mixed': 96}\n",
      "  mixed: 96 experiments (32 base × 3 reps)\n",
      "\n",
      "Executing 96 experiments...\n",
      "\n",
      "🔄 STAGE 1: Batch Data Generation\n",
      "🔄 Generating data for 96 experiments on GPU (CuPy)…\n",
      "  Processing group 1/16\n",
      "  Processing group 6/16\n",
      "  Processing group 11/16\n",
      "  Processing group 16/16\n",
      "✅ Generated data for 96 valid experiments (GPU)\n",
      "✅ Generated 96 datasets\n",
      "\n",
      "🏆 STAGE 2: Interpretable Model vs Black Box Evaluation\n",
      "GPU Memory Stage 2 Memory Check: 3313.6MB used, 12989.0MB free (16302.6MB total)\n",
      "Using batch size: 48 (based on 12989MB available)\n",
      "  Processing batch 1/2 (48 model comparisons)\n",
      "GPU cleanup completed\n",
      "    Exp   1 (mixed→MAPS): AMS=0.928 RF=0.856 NN=0.701 Status=STRONG Interp=0.974\n",
      "GPU cleanup completed\n",
      "GPU Memory Batch progress 10/48: 3329.6MB used, 12973.0MB free (16302.6MB total)\n",
      "    Exp  53 (mixed→MAPS): AMS=0.911 RF=0.755 NN=0.371 Status=STRONG Interp=0.949\n",
      "GPU cleanup completed\n",
      "GPU Memory Batch progress 20/48: 3329.6MB used, 12973.0MB free (16302.6MB total)\n",
      "    Exp  12 (mixed→MAPS): AMS=0.848 RF=0.787 NN=0.331 Status=STRONG Interp=0.986\n",
      "GPU cleanup completed\n",
      "GPU Memory Batch progress 30/48: 3329.6MB used, 12973.0MB free (16302.6MB total)\n",
      "    Exp  16 (mixed→MAPS): AMS=0.871 RF=0.690 NN=0.489 Status=STRONG Interp=0.973\n",
      "GPU cleanup completed\n",
      "GPU Memory Batch progress 40/48: 3329.6MB used, 12973.0MB free (16302.6MB total)\n",
      "    Exp  68 (mixed→MAPS): AMS=0.942 RF=0.721 NN=-0.122 Status=STRONG Interp=0.953\n",
      "GPU cleanup completed\n",
      "  Processing batch 2/2 (48 model comparisons)\n",
      "GPU cleanup completed\n",
      "    Exp  25 (mixed→MAPS): AMS=0.877 RF=0.849 NN=0.301 Status=STRONG Interp=0.961\n",
      "GPU cleanup completed\n",
      "GPU Memory Batch progress 10/48: 3329.6MB used, 12973.0MB free (16302.6MB total)\n",
      "    Exp  77 (mixed→MAPS): AMS=0.927 RF=0.775 NN=0.412 Status=STRONG Interp=0.951\n",
      "GPU cleanup completed\n",
      "GPU Memory Batch progress 20/48: 3329.6MB used, 12973.0MB free (16302.6MB total)\n",
      "    Exp  36 (mixed→MAPS): AMS=0.938 RF=0.681 NN=0.386 Status=STRONG Interp=0.899\n",
      "GPU cleanup completed\n",
      "GPU Memory Batch progress 30/48: 3329.6MB used, 12973.0MB free (16302.6MB total)\n",
      "    Exp  40 (mixed→MAPS): AMS=0.951 RF=0.875 NN=0.689 Status=STRONG Interp=0.977\n",
      "GPU cleanup completed\n",
      "GPU Memory Batch progress 40/48: 3329.6MB used, 12973.0MB free (16302.6MB total)\n",
      "    Exp  92 (mixed→MAPS): AMS=0.915 RF=0.805 NN=0.262 Status=STRONG Interp=0.958\n",
      "GPU cleanup completed\n",
      "\n",
      "📊 STAGE 3: Interpretable Model Results Analysis\n",
      "\n",
      "================================================================================\n",
      "INTERPRETABLE MODEL DOE RESULTS\n",
      "================================================================================\n",
      "🏆 Completed 96 model comparisons\n",
      "⏱️  Total runtime: 9.4 minutes\n",
      "\n",
      "================ RESULTS SUMMARY ================\n",
      "Rows: 96\n",
      "\n",
      "Status level counts:\n",
      "  COMPETITIVE     : 2 (2.1%)\n",
      "  STRONG          : 94 (97.9%)\n",
      "\n",
      "Method selection counts:\n",
      "  MAPS            : 96 (100.0%)\n",
      "Mean ams_r2: 0.9125\n",
      "Mean benchmark_r2_max: 0.7952\n",
      "Mean ams_interpretability_score: 0.9576\n",
      "✅ Mixed results saved: 96 experiments\n",
      "GPU cleanup completed\n",
      "GPU Memory Final cleanup: 3329.6MB used, 12973.0MB free (16302.6MB total)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import os\n",
    "    import time\n",
    "    \n",
    "    print(\"=== RUN 3: MIXED FUNCTIONS ===\")\n",
    "    gpu_memory_check(\"Initial\")\n",
    "    \n",
    "    # Run mixed only\n",
    "    df_mixed = run_interpretable_experiments(function_type=\"mixed\")\n",
    "    \n",
    "    # Save immediately\n",
    "    df_mixed_pandas = df_mixed.to_pandas()\n",
    "    df_mixed_pandas.to_csv(\"/mnt/c/users/lfult/OneDrive - bc.edu/desktop/aps/results_mixed.csv\", index=False)\n",
    "    print(f\"✅ Mixed results saved: {len(df_mixed)} experiments\")\n",
    "    \n",
    "    # Clean up\n",
    "    del df_mixed\n",
    "    gpu_cleanup(verbose=True)\n",
    "    cp.get_default_memory_pool().free_all_blocks()\n",
    "    gpu_memory_check(\"Final cleanup\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11cbe45-25d3-44dc-8a03-8579b6c04681",
   "metadata": {},
   "source": [
    "# Combined Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "41bbee95-9822-4b2c-ae25-9ee6223d4da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== COMBINING ALL RESULTS ===\n",
      "📊 FINAL COMBINED RESULTS:\n",
      "  Additive: 96 experiments\n",
      "  Multiplicative: 96 experiments\n",
      "  Mixed: 96 experiments\n",
      "  Total: 288 experiments\n",
      "\n",
      "================ RESULTS SUMMARY ================\n",
      "Rows: 288\n",
      "\n",
      "Status level counts:\n",
      "  COMPETITIVE     : 14 (4.9%)\n",
      "  INTERPRETABLE_ONLY: 5 (1.7%)\n",
      "  NEAR            : 3 (1.0%)\n",
      "  STRONG          : 252 (87.5%)\n",
      "  WEAK            : 14 (4.9%)\n",
      "\n",
      "Method selection counts:\n",
      "  APS             : 82 (28.5%)\n",
      "  MAPS            : 206 (71.5%)\n",
      "Mean ams_r2: 0.8913\n",
      "Mean benchmark_r2_max: 0.7766\n",
      "Mean ams_interpretability_score: 0.9610\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import pandas as pd\n",
    "    \n",
    "    print(\"=== COMBINING ALL RESULTS ===\")\n",
    "    \n",
    "    # Load all three CSV files\n",
    "    df_add = pd.read_csv(\"/mnt/c/users/lfult/OneDrive - bc.edu/desktop/aps/results_additive.csv\")\n",
    "    df_mult = pd.read_csv(\"/mnt/c/users/lfult/OneDrive - bc.edu/desktop/aps/results_multiplicative.csv\")\n",
    "    df_mixed = pd.read_csv(\"/mnt/c/users/lfult/OneDrive - bc.edu/desktop/aps/results_mixed.csv\")\n",
    "    \n",
    "    # Combine\n",
    "    results_df = pd.concat([df_add, df_mult, df_mixed], ignore_index=True)\n",
    "    \n",
    "    # Save final combined results\n",
    "    results_df.to_csv(\"/mnt/c/users/lfult/OneDrive - bc.edu/desktop/aps/results_combined.csv\", index=False)\n",
    "    \n",
    "    print(f\"📊 FINAL COMBINED RESULTS:\")\n",
    "    print(f\"  Additive: {len(df_add)} experiments\")\n",
    "    print(f\"  Multiplicative: {len(df_mult)} experiments\") \n",
    "    print(f\"  Mixed: {len(df_mixed)} experiments\")\n",
    "    print(f\"  Total: {len(results_df)} experiments\")\n",
    "    \n",
    "    # Convert to cuDF for your analysis functions\n",
    "    results_cudf = cudf.from_pandas(results_df)\n",
    "    analyze_results(results_cudf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022c90c9-fb8b-4154-b0e2-200740d643d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch-env)",
   "language": "python",
   "name": "torch-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
